%----------------------------------------------------
% 18 September 2024 | New introduction for submission to JBF
%----------------------------------------------------

In financial markets, the ability to efficiently process and react to new information is fundamental. Markets are constantly processing vast amounts of data, much of which stems from media coverage that reports on firm-specific events, macroeconomic developments, and geopolitical shocks. Historically, understanding how news influences stock prices has been a central concern in finance research. The Efficient Market Hypothesis (EMH) posits that markets reflect all available information, including news, instantaneously and efficiently (Fama, 1970). However, both theoretical and empirical studies have shown that this process is not always immediate or efficient, particularly when news is complex or ambiguous (Klibanoff, Lamont \& Wizman, 1998).

The role of media in driving market reactions has been well documented. Early work by Tetlock (2007) demonstrated that negative words in news articles could predict declines in stock prices, laying the foundation for textual analysis in finance. Building on this, Fang and Peress (2009) explored the relationship between media coverage and stock returns, showing that firms with less media coverage tend to generate higher future returns. These studies established the connection between textual content and market behavior, particularly through sentiment analysis. However, sentiment alone may not fully capture the complexity of news content, as Jegadeesh and Wu (2013) emphasized by demonstrating the importance of refining textual analysis to capture nuanced information.

Traditional models of market reactions have relied heavily on structured data like earnings reports, which are easier to quantify. However, the richness of unstructured data-like news articles-has increasingly attracted attention. Studies such as those by Bollen et al. (2011) on social media sentiment and Barber \& Odean (2008) on investor behavior highlight the expanding role of sentiment in understanding market trends. Yet, sentiment-based approaches, while informative, often fail to capture the economic implications of news in its full complexity.

Recent advancements in machine learning, particularly in NLP, have opened up new possibilities for more granular analysis of news content. Studies like Bybee et al. (2021) employed Latent Dirichlet Allocation (LDA) to distilling news articles into thematic categories. Their findings demonstrated that shifts in thematic attention correlate closely with macroeconomic activity, explaining a significant portion of aggregate stock market returns. However, the limitation of their approach, which uses a bag-of-words model, is that it overlooks the contextual and syntactic nuances of language. In contrast, newer techniques such as those employed in this paper, leveraging the capabilities of Large Language Models (LLMs), preserve these nuances, allowing for a more precise understanding of how news content influences individual stock prices.

While much of the existing literature has concentrated on sentiment-based approaches (Engle, 2019; Garcia, 2013), this paper seeks to push the boundaries by applying advanced NLP techniques to extract deeper insights from news articles. Specifically, we investigate whether clustering news articles based on their economic implications-such as demand, supply, financial, policy, and technology shocks-, rather than simply their sentiment, can enhance the predictive power of trading strategies. By employing clustering methods such as KMeans, which groups news articles based on text embeddings, and Large Language Models (LLMs), which classify articles using a predefined schema to detect firm-specific shocks, we aim to offer a more nuanced approach that captures the economic relevance of news to particular firms.

The key distinction between these approaches lies in the level of understanding they offer. KMeans clustering relies on unsupervised grouping based on article embeddings, capturing broad similarities in textual content. In contrast, LLM-based clustering involves a structured analysis of the news, where the model is explicitly tasked with classifying the shocks that affect specific firms. This deeper parsing results in clusters that are more stable across time and splits, generating cleaner trading signals that are more economically interpretable.

The integration of LLMs has seen growing adoption in finance, as demonstrated by Chen et al. (2022), who explore LLM-based market timing strategies. However, they relied on outdated models like BERT and RoBERTa, which limit adaptability. In contrast, our approach employs cutting-edge models (e.g., LLaMA-3, released in 2024) and function-calling to extract firm-specific news signals, ensuring higher relevance for market predictions. Similarly, while Lopez-Lira and Tang (2023) used ChatGPT to predict stock price movements based on news headlines, their analysis was confined to simplistic classifications of headline sentiment. Our methodology, which parses entire news articles rather than headlines, captures deeper economic context and generates more actionable trading signals. 

Moreover, works like Bybee (2023) show that LLMs can be effective tools for generating economic beliefs from historical news data. While his analysis focused on replicating survey-based economic expectations, our paper adopts a more targeted approach by focusing on firm-specific news impacts, offering practical applications for investors. Similarly, Scherbina and Schlusche (2016) identified how news stories reveal economic linkages between firms, finding that linked firms cross-predict one another's returns. This highlights the broader relevance of news in forecasting firm performance, a theme we expand upon by clustering news based on specific economic shocks.

The novelty of this paper, therefore, lies in demonstrating how a deeper understanding of news articles, facilitated by LLMs, can lead to more stable clusters, improved signal clarity, and enhanced out-of-sample performance. Our findings suggest that LLM-based clustering of news provides a more structured and economically relevant lens through which market participants can interpret news, offering a potential edge in constructing profitable trading strategies.

The ability to identify firm-specific impacts in a structured manner is critical, as Hilt \& Schwenkler (2024) argue that networks of firm linkages-extracted from news-are crucial for understanding firm-level and aggregate risks. Our use of LLMs aligns with this line of research, offering a scalable and consistent methodology to parse news and generate actionable insights. Finally, while previous studies like Chen (2023) focused on market-level predictions from headlines, our firm-level approach allows for greater granularity, improving the precision of market timing strategies.

In conclusion, this paper builds on the foundation of textual analysis in finance, extending it into the realm of advanced NLP and LLMs. By parsing news into economic shocks and clustering articles accordingly, we demonstrate that LLM-based clustering methods outperform traditional approaches in generating cleaner, more stable signals for trading strategies. This contributes to a growing body of literature that seeks to understand how unstructured data, particularly news, can be harnessed to predict market movements with greater accuracy and nuance.

%While much of the literature has focused on sentiment-based approaches (Engle, 2019; Garcia, 2013), we seek to contribute to a growing field of research that applies advanced NLP techniques to extract deeper insights from news content. Specifically, we investigate whether clustering news articles based on their economic implications, rather than simply their sentiment, can enhance the predictive power of trading strategies. By employing clustering techniques such as KMeans on text embeddings and Large Language Models (LLMs), we can offer a more nuanced approach that captures the economic relevance of news to particular firms.
%
%The novelty of this paper lies in the comparison of clustering methods based on two distinct approaches: traditional KMeans clustering on text embeddings, and clustering based on Large Language Models (LLMs), which offer a deeper, more structured understanding of the news content. By allowing the LLM to parse articles and classify them according to shock types, we argue that the resulting clusters are more stable over time, produce cleaner trading signals, and ultimately lead to better out-of-sample performance. This approach provides a more structured, economically-relevant lens through which market participants can interpret news, offering a potential edge in constructing a profitable trading strategy.


%----------------------------------------------------
% 18 June 2024 | Raw text from my own writing
%----------------------------------------------------

\hspace{0.5cm} This paper aims to provide a novel and universal approach to analyzing the impact of business news on stock prices. Our approach is novel in that it is the first time that Large Language Models are employed to \textit{comprehensively} analyze the shocks described in business news articles for return prediction purposes, and it is universal in the sense that it does not rely on access to structured metadata from paid news portals, which, in general, are not widely accessible to the common researcher.

\mx 
Our database consists of a set of Spanish business news articles from DowJones spanning June 2020 to September 2021. Such articles are filtered in a way that allows us to extract the firms directly involved in them. In the first exercise, we convert the wording of the articles from text to high-dimensional vector embeddings by using a transformer model. Such representation captures the general contextual and semantic meaning of the text but is not able to capture the subtle nuances of the shocks described there on the affected firms. 

\mx 
We then cluster our news articles by applying the KMeans algorithm to the associated vector embeddings. This procedure delivers 26 clusters of business news, where each cluster usually pools together articles about a firm or set of firms in the same sector. 

\mx 
For each firm affected by an article, a market model is constructed on some window previous to the day where the article's information got incorporated into the market. Such model is then used to construct a beta-neutral strategy that extracts the abnormal returns of the firm after controlling for the market. 
%\mx 
By obtaining the metrics of this strategy and comparing them across clusters, we can obtain a measure of the profitability of each cluster. We then propose two algorithms that exploit this information to select the optimal clusters.
%to build a trading rule. 

\mx 
Finally, a trading rule is constructed by launching trades on the selected clusters over a specific holding period. By projecting the trading rule onto the test set, we obtain a measure of the profitability of the whole procedure out-of-sample. 

\mx 
The strategy based on KMeans clustering of vector embeddings is not able to generate a consistent earnings profile in the test set, which occurs due to the instability of the clustering method. Namely, the distribution of articles through clusters across data splits shows a very inconsistent pattern, which already hints at the fact that signals generated by the trading rule will not be generalizable out-of-sample.

\mx 
In the second part of the paper, we feed the news articles to a Large Language Model (LLM) and ask it to manually parse them. In particular, we ask the LLM to extract the affected firms and to individually classify the shock implied by the article in each affected firm based on a predefined schema. Such schema consists of a classification of news articles' shocks based on three categories: type (demand, supply, financial, technology, policy), magnitude (minor, major), and direction (positive, negative). We can then cluster the articles based on this classification. 

\mx
In this case, the distribution of articles through clusters is very stable across data splits, which indicates that the trading rule will generate signals that will be generalizable across data splits. Indeed, this is confirmed by the out-of-sample performance of the trading strategy, which shows a consistent earnings profile. These results are robust to hyperparameter variability. In particular, we show that the distribution of Sharpe Ratios in the test set for different choices of holding period length and maximum number of traded clusters is right-skewed and centered at positive values.

%----------------------------------------------------
% 29 April 2024 | Raw text from my own writing
%----------------------------------------------------

%This paper aims to provide a novel and universal approach to analyzing the impact of business news on stock prices. Our approach is novel in that it is the first time that Large Language Models are employed to comprehensively analyze the shocks described in business news articles for return prediction purposes, and it is universal in the sense that it does not rely on access to structured metadata from paid news portals, which, in general, are not widely accessible to the common researcher.


%
%\mx 
%We start with an unstructured corpus of textual data, namely Spanish business news articles, which undergoes preprocessing before being fed into the GPT API. Subsequently, we guide GPT in parsing these news articles and generating structured responses using "function calling", i.e., a predefined set of functions and parameters of our writing to rigorously instruct GPT on how to respond. 
%
%\mx
%Such functions prompt GPT to identify various attributes of the article, including its publication datetime, the type of information provided (new information, historical, analysis/comments, marketing) and its scope (firm-level, industry, global). If the scope of the article is at the firm level, we further ask GPT to list the firms that are directly affected by the events narrated therein. For each identified firm, we request GPT to provide its associated stock market ticker (if publicly traded; otherwise, it returns "NaN"), and further, to classify several aspects regarding how the shock pertains to the firm. Specifically, we prompt GPT to classify the type of shock (e.g., demand, supply, regulatory), the expected duration (short-term, long-term, permanent), the magnitude or relevance (minor, mild, major), and the expected impact on the firm's performance (positive, neutral, negative). Lastly, we obtain GPT's own trading signal by putting it in the shoes of a financial advisor tasked with deciding whether to \textit{Long} or \textit{Short} the stock associated with the firm affected by the news article.
%
%\mx 
%The methodological framework outlined above not only facilitates the identification of pertinent metadata but also generates a structured and comparable array of responses, enabling us to progress the analysis to a supervised stage, which consists of two parts.
%
%\mx 
%In the first part, we examine the predictability of stock returns in response to the shocks delineated in the news articles. To achieve this, we will conduct regressions of the stock returns of affected firms on the dummified shock classifications provided by GPT. This analysis will elucidate the direction and significance of shock predictors for return prediction.
%
%\mx 
%In the second part, we will assess the market timing capabilities of GPT through a market timing test. This involves contrasting GPT's decisions with the decisions that should have been made based on realized returns. In other words, we will juxtapose the stock return predictions of GPT, which inform decisions to long or short the stock, with the actual stock return performance of the respective stock.
%
%\mx 
%Finally, we will construct a set of Long-Short portfolios. One of these portfolios will trade shock signals, determining whether to go long or short based on the shock classifications provided by GPT. Another portfolio will be created using GPT's raw market timing capabilities, disregarding the deeper understanding of the news article implied by the shock analysis and categorization.
%
%\mx 
%All in all, our approach allows us to transition from an unsupervised learning procedure with unstructured data to a supervised learning procedure with structured data that enables us to study the stock return predictability of the shocks described by business news articles.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%\mx 
%%The methodological framework described above not only facilitates the identification of relevant metadata but also produces a structured and comparable set of responses that allows us to advance the analysis further to a supervised stage,
%
%
%%In the first part, we analyze the stock return predictability of the shocks described in the news articles. For this purpose, we will regress the stock returns of the affected firms on the dummified shock classifications made by GPT. This will allow us to shed light on the direction and significance of shock predictors for return prediction. 
%%
%%For the second part, we will analyze the market timing capabilities of GPT by performing a market timing test. Here we will contrast the decision made by GPT with the decision that should have been taken based on the realized returns. In n other words, we will compare the stock return predictions of GPT (which underlie in the decision made to long or short the stock) and the actual stock return performance of the stock in question. 
%
%%Finally, we will construct a set of Long-Short portfolios. One of such portfolio will trade shock signals; that is, it will decide whether to long/short based on the shock classifications made by GPT. Another portfolio will be constructed using GPT's raw market timing abilities (that is, by simply asking GPT to long or short based on the news and without regard to the deeper understanding on the news article implied by the shock analysis and categorization)
%
%
%%in which we study the stock return predictability of the shock analysis and market timing signals generated by GPT.
%
%%Namely, the output from GPT's response consists of a classification of the shocks implied by the news articles. We launch a set of queries to GPT: we ask it to identify the publication datetime of the article, the type of article (set of possible answers: new information, historical, analysis or comments, marketing) and its scope (firm-level, industry, global). If the scope of the article is at the firm-level, then, we ask GPT to list the firms that are primarily and directly affected by the events narrated in the article. Then, for each firm in that list of firms, we prompt GPT to give us its associated stock market ticker (if the company is publicly traded, otherwise, it spits ``NaN'') and we further ask GPT to classify a set of aspects of how that shock relates to the firm in question. In particular, we ask it about the shock type (demand, supply, regulation,...), the expected duration of that shock (set of possible answers: short-term, long-term, permanent), the magnitude or relevance (set of possible answers: minor, mild, or major), and the direction in which that shock is expected to affect the firm's performance (set of possible answers: positive, neutral negative). Further, we ask GPT to provide a trading signal based on the news article; namely, we put GPT in the shoes of a financial advisor having to decide on whether to Long or Short the stock associated to the affected firm based on the events described in the news article. Once we have obtained a structured answer from GPT we can obtain the metadata of the identified firms to analyze the stock return predictability of the shock analysis and market timing signals generated by GPT.
%%
%
%
%
%%In particular, our approach departs from an unstructured dataset of textual data (a corpus of Spanish news articles) that is preprocessed and fed to GPT. At this stage, we direct GPT on how to parse these news articles and generate a structured response through ``function calling'' (i.e., writing a set of functions and parameters to precisely instruct GPT on to output its completions). This methodology allows us to not only identify the relevant metadata but also to obtain a structured and comparable output that places us in the right place to take the analysis to a supervised stage.
%%---
%%
%%At this stage, a set of functions is defined to instruct GPT on how to parse and analyze the news articles. 
%%
%%At this stage, we task GPT to produce a set of answers according to a function schema that we predefined in advance. 
%%
%%
%%---
%
%%
%%. Different from the previous literature, our procedures don't rely on having access to structured metadata from payable news portals, which are not widely accessible to the common researcher. 
%%
%%In other words, our approach is universal in that it doesn't require metadata from news portals that require subscriptions. 
%%
%%In this sense, we obtain all the information by asking GPT. This is an unsupervised learning approach. 
%%
%%How do we get around this? By delegating the identification of all the relevant metadata to GPT. However, this delegation only works if the right textual preprocessing is performed and the right questions are asked to GPT.  
%%
%%The modus operandi consists of starting from an unsupervised learning approach with unstructured textual data (news articles), which we then preprocess and feed to GPT. 
%%
%%Departing from unstructured news article data, we parse those articles by passing them through the GPT API. 
%%
%%
%%This leads GPT to produce a structured output that can then be employed to analyze the stock return predictability of GPT. 
%%
%
%
%
%%Namely, the output from GPT's response consists of a classification of the shocks implied by the news articles. We launch a set of queries to GPT: we ask it to identify the publication datetime of the article, the type of article (set of possible answers: new information, historical, analysis or comments, marketing) and its scope (firm-level, industry, global). If the scope of the article is at the firm-level, then, we ask GPT to list the firms that are primarily and directly affected by the events narrated in the article. Then, for each firm in that list of firms, we prompt GPT to give us its associated stock market ticker (if the company is publicly traded, otherwise, it spits ``NaN'') and we further ask GPT to classify a set of aspects of how that shock relates to the firm in question. In particular, we ask it about the shock type (demand, supply, regulation,...), the expected duration of that shock (set of possible answers: short-term, long-term, permanent), the magnitude or relevance (set of possible answers: minor, mild, or major), and the direction in which that shock is expected to affect the firm's performance (set of possible answers: positive, neutral negative). Further, we ask GPT to provide a trading signal based on the news article; namely, we put GPT in the shoes of a financial advisor having to decide on whether to Long or Short the stock associated to the affected firm based on the events described in the news article. Once we have obtained a structured answer from GPT we can obtain the metadata of the identified firms to analyze the stock return predictability of the shock analysis and market timing signals generated by GPT.
%
%
%% That is, our approach allows us to transition from an unsupervised learning with unstructured data to a supervised learning procedure with structured data. This latter procedure consists of two parts.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%----------------------------------------------------
%% 29 April 2024 | Parsed using GPT (asked it to simply rephrase my text)
%%----------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%This paper endeavors to establish a methodical framework for the analysis of business news articles, aiming to transform unstructured news data into structured insights through the utilization of the GPT API.
%%
%%By instructing GPT through a series of predefined functions, we orchestrate the parsing of news articles to yield structured responses. This approach facilitates the extraction of comparable insights, primarily focusing on classifying the shocks conveyed within the articles. Our methodology involves querying GPT to ascertain key attributes of the articles, including publication datetime, article type (such as new information, historical context, analysis, or commentary), and scope (ranging from firm-level to industry or global perspectives).
%%
%%When the scope pertains to specific firms, we prompt GPT to identify the principal firms directly impacted by the events narrated in the article. Subsequently, for each identified firm, we solicit GPT to classify various aspects of the shock, encompassing its type (such as demand, supply, or regulatory), expected duration (ranging from short-term to long-term or permanent), magnitude or relevance (ranging from minor to major), and anticipated direction of impact on the firm's performance (positive, neutral, or negative). Additionally, we task GPT with generating trading signals based on the news articles, simulating the role of a financial advisor deciding whether to Long or Short the stock associated with the affected firm based on the article's content.
%%
%%The initial phase of our study focuses on analyzing the predictability of stock returns based on these shock classifications. Employing regression analysis, we explore the relationship between stock returns of affected firms and the identified shock predictors, examining both the direction and significance of these predictors.
%%
%%Subsequently, we delve into assessing the market timing capabilities of GPT through a rigorous market timing test. This involves contrasting GPT's decisions regarding stock returns with the ideal decisions that should have been made, thereby evaluating the effectiveness of GPT's predictions in real-world market scenarios.
%%
%%In the final phase, we construct a series of Long-Short portfolios to further explore the practical implications of GPT's insights. One such portfolio will trade based on shock signals, leveraging GPT's shock classifications to inform Long or Short decisions. Another portfolio will utilize GPT's raw market timing abilities, disregarding deeper shock analysis, and solely relying on GPT's instantaneous market predictions derived from news articles.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%----------------------------------------------------
%% 29 April 2024 | Parsed using GPT (asked it to do whatever it want with my text)
%%----------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%This paper presents a systematic framework for analyzing business news articles using the GPT API. Departing from unstructured data, we employ a structured approach by instructing GPT through predefined functions, enabling the extraction of comparable and analyzable responses.
%%
%%Our methodology begins by parsing articles through the GPT API, directing it to identify key attributes such as publication datetime, article type, and scope. For articles focused on individual firms, GPT identifies the primary affected firms and classifies aspects of the associated shock, including type, duration, magnitude, and expected impact direction.
%%
%%Furthermore, we task GPT with providing trading signals based on the news content, simulating the decision-making process of a financial advisor. This approach lays the foundation for our subsequent analyses.
%%
%%In the first part of the paper, we investigate the predictability of stock returns based on GPT's shock classifications. Through regression analysis, we examine the significance and directionality of these predictors.
%%
%%Subsequently, we assess GPT's market timing capabilities by comparing its stock return predictions with actual performance. This market timing test evaluates the efficacy of GPT's decision-making process in real-world scenarios.
%%
%%Finally, we construct Long-Short portfolios to further explore GPT's capabilities. One portfolio trades based on GPT's shock signals, while another relies solely on GPT's raw market timing abilities, providing insights into the impact of deeper news analysis.
%%
%%By employing this structured approach, we aim to enhance our understanding of the relationship between business news content and stock market movements, leveraging the capabilities of advanced language models like GPT. 
%%
%%
%%
%%
