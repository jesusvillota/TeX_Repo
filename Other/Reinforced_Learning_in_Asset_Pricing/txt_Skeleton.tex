\section{Introduction}

\subsection{Motivation}

Reinforcement Learning (RL) has proven successful in solving dynamic decision-making problems across a wide range of domains, from robotics to game theory. In financial markets, RL's ability to adapt to evolving market conditions and learn optimal strategies over time positions it as a powerful tool for asset pricing, portfolio optimization, and risk management.

The existing paper by \textit{Chen, Pelger, and Zhu} uses deep learning techniques such as \textit{Generative Adversarial Networks (GANs)} to estimate the Stochastic Discount Factor (SDF), which captures the pricing of systematic risks. However, their approach is static in nature and focuses on minimizing pricing errors based on conditional moments at each point in time. Incorporating RL allows the model to \textit{dynamically learn} how the SDF evolves over time by continuously interacting with financial data, optimizing pricing decisions, and dynamically adjusting the portfolio to maximize long-term returns.

\subsection{Contribution}

We propose extending the deep learning asset pricing framework to integrate \textit{Reinforcement Learning}. By doing so, we enable the SDF model to not only minimize pricing errors but also \textit{optimize policies} that adjust the SDF dynamically based on market feedback. Specifically, our model learns to optimally adjust the \textit{SDF portfolio weights} and \textit{risk exposures} $\beta_t$ over time to maximize a reward function that reflects \textit{risk-adjusted returns} and \textit{pricing accuracy}.

\section{Methodology}

\subsection{Reinforcement Learning Framework for Asset Pricing}

\subsubsection{Agent and Environment}
In RL, the \textit{agent} (representing the asset pricing model) interacts with the \textit{environment} (the financial market) by observing state variables (macroeconomic factors, firm-specific characteristics) and taking actions (adjusting the SDF portfolio weights $\omega_t$ or selecting assets to hold). The agent receives \textit{rewards} (e.g., Sharpe ratio or risk-adjusted returns) and updates its policy to improve long-term outcomes.

\subsubsection{State Space ($s_t$)}
The state $s_t$ includes all relevant information at time $t$ for pricing assets. This includes macroeconomic variables ($I_t$), firm-specific characteristics ($I_{t, i}$), and the SDF portfolio weights from the previous time step ($\omega_{t-1}$). By incorporating \textit{LSTM layers}, the model captures both \textit{short-term} and \textit{long-term dependencies} in macroeconomic and firm-specific variables, as in the original paper.

\subsubsection{Action Space ($a_t$)}
Actions correspond to adjusting the \textit{SDF portfolio weights} $\omega_t$ or selecting assets and risk exposures $\beta_t$. These actions aim to \textit{optimize the SDF} and adjust the exposure to systematic risks dynamically. The action space can be continuous (adjusting weights incrementally) or discrete (selecting specific portfolios or factors).

\subsubsection{Reward Function}
The reward function should reflect the trade-off between maximizing \textit{risk-adjusted returns} and minimizing \textit{pricing errors}. One possible formulation is:

\[
R_t = \text{Sharpe Ratio of the SDF portfolio} - \lambda \cdot \text{Pricing Error}
\]

The \textit{Sharpe ratio} rewards the agent for high returns relative to risk, while the \textit{pricing error} penalty discourages mispricing. The parameter $\lambda$ controls the trade-off between these objectives.

\subsubsection{Policy Update}
The agent's policy is updated using techniques like \textit{Q-learning}, \textit{Deep Q-Networks (DQN)}, or \textit{Policy Gradient Methods}. The policy determines how actions are taken based on the current state $s_t$. Using \textit{Deep Reinforcement Learning (DRL)}, the policy is parameterized by a neural network that approximates the optimal policy.

\subsection{Model Architecture}

\subsubsection{State Representation}
Similar to the original paper, we use an \textit{LSTM} to encode macroeconomic dynamics into a \textit{state process} $h_t$. The LSTM processes the macroeconomic time-series data and captures hidden cyclical patterns relevant for asset pricing.

\subsubsection{Actor-Critic Architecture}
We adopt an \textit{actor-critic} RL architecture:
\begin{itemize}
    \item The \textit{actor} network outputs the \textit{SDF portfolio weights} $\omega_t$, which represent the action taken by the model.
    \item The \textit{critic} network evaluates the value of the state-action pair, providing feedback to the actor about the quality of the current action.
\end{itemize}

\subsubsection{GAN Integration}
The adversarial network used in the original paper to find difficult-to-price conditions can be maintained as part of the \textit{environment} in which the RL agent operates. The adversary would continue to maximize the pricing errors, forcing the RL agent to improve its policy over time.

\subsubsection{Regularization}
To avoid overfitting, \textit{dropout} and \textit{adaptive learning rates} can be used as in the original paper. Regularization techniques like \textit{Elastic Net} can be incorporated to deal with a high-dimensional characteristic space.

\subsection{Learning Dynamics}

\subsubsection{Exploration vs. Exploitation}
In RL, there is a fundamental trade-off between \textit{exploration} (trying new portfolio strategies or adjustments to improve the SDF) and \textit{exploitation} (sticking with the current best-known strategy). The model can use techniques like \textit{epsilon-greedy} or \textit{softmax exploration} to balance this trade-off.

\subsubsection{Training Process}
The model is trained by simulating multiple episodes of asset pricing, where the RL agent interacts with the environment and updates its policy based on the observed rewards. Training should include validation on out-of-sample data to avoid overfitting and ensure robustness.

\section{Evaluation}

\subsection{Comparison with GAN and FFN Models}
The performance of the RL-based model can be compared with the GAN and FFN models from the original paper using the same three evaluation metrics:
\begin{enumerate}
    \item \textbf{Sharpe Ratio} of the SDF portfolio.
    \item \textbf{Explained Variation} (EV).
    \item \textbf{Cross-sectional $R^2$} (XS-$R^2$).
\end{enumerate}

\subsection{Portfolio-Level Evaluation}
Evaluate the RL model on \textit{characteristic-sorted portfolios} and compare its ability to capture the loading structure across different market conditions.

\subsection{Dynamic Behavior}
Assess how well the RL model adapts to changing market dynamics, especially in periods of high volatility or economic shocks, compared to static models like GAN and FFN.

\section{Conclusion}
The integration of \textit{Reinforcement Learning} into the asset pricing framework allows the SDF to evolve dynamically and learn optimal strategies based on market feedback. This extension builds on the strengths of the original GAN model while addressing its limitations in capturing dynamic market behavior. RL's ability to \textit{adapt over time} and optimize long-term returns while minimizing pricing errors makes it a powerful tool for next-generation asset pricing models.

\section{Technical Appendices}
\begin{itemize}
    \item \textbf{Appendix A}: Details on the RL algorithm (e.g., Q-learning, DDPG) and how it is implemented in the asset pricing context.
    \item \textbf{Appendix B}: Discussion on hyperparameter tuning for the RL model (e.g., learning rates, exploration parameters).
    \item \textbf{Appendix C}: Simulation results comparing RL, GAN, and FFN models across various market conditions.
\end{itemize}
