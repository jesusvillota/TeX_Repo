%----------------------------------------------------
\section{Introduction}
%----------------------------------------------------
In financial markets, news play a pivotal role in shaping stock prices. Every day, market participants respond to a broad spectrum of news ranging from firm-specific announcements, such as earnings releases, to macroeconomic events, such as central bank interest rate announcements, or geopolitical developments, like international trade conflicts or political elections. The Efficient Market Hypothesis (EMH), formalized by 
\cite{fama1970efficient} Fama (1970), 
posits that markets efficiently incorporate new information almost instantaneously. Both theoretical perspectives and empirical observations indicate that markets do not always exhibit such efficiency, particularly when the information is complex or ambiguous. This discrepancy between theory and reality suggests significant room for improvement in understanding how news is processed by market participants and how it influences asset prices.
%
A substantial body of literature has tried to predict market reactions to news, yet some important gaps persist. Our review of the literature reveals three critical limitations in current approaches to analyzing financial news: a lack of economic focus in textual analysis methodology, insufficient attention to firm-specific effects, and over-reliance on headlines.

\mx 
%----------------------------------------------------
First, we examine the lack of economic focus in current methodological approaches to analyzing financial news. This limitation is evident across three main streams of literature.
\begin{quote}
\hspace{0.5cm} \textit{Sentiment Analysis.} 
Traditional approaches frequently rely on sentiment analysis, reducing the richness of news content to binary classifications of positive or negative sentiment. The seminal work of 
\cite{tetlock2007giving} Tetlock (2007) 
demonstrated the predictive power of media sentiment in financial markets, showing that negative media coverage leads to downward pressure on market prices, followed by a reversion to fundamentals. This finding sparked significant interest in sentiment-based approaches, with 
\cite{tetlock2008more} Tetlock et al. (2008) 
extending the analysis to firm-specific news and revealing that negative word content not only forecasts poor firm earnings but also indicates a temporary underreaction in stock prices.
Despite these early successes, the methodology of sentiment analysis has faced important challenges. 
%
\cite{loughran2011liability}  Loughran and McDonald (2011) 
highlighted a fundamental issue: general-purpose dictionaries often misclassify words in financial contexts, leading them to develop specialized financial word lists. Building on this insight, 
\cite{jegadeesh2013word}  Jegadeesh and Wu (2013) 
demonstrated that the weighting scheme applied to these words is as crucial as the word lists themselves, introducing a more nuanced approach to content analysis.
The emergence of social media and machine learning has driven further methodological innovations in sentiment analysis. 
\cite{bollen2011twitter}  Bollen et al. (2011) 
leveraged Twitter data to predict DJIA movements, while 
\cite{garcia2013sentiment}  Garcia (2013) 
revealed that sentiment's predictive power is particularly pronounced during recessions, suggesting time-varying importance of news sentiment. Recent advances in machine learning have pushed the boundaries further, with 
\cite{ke2019predicting}  Ke et al. (2019) 
developing a sophisticated supervised learning framework specifically designed for return prediction. The advent of transformer-based models has enabled even more sophisticated approaches, with 
\cite{lee2020bert}  Lee et al. (2020) 
and 
\cite{wei2018stock}  Wei and Nguyen (2020) 
applying BERT-based architectures to financial sentiment analysis.
%
However, despite their widespread adoption and continued methodological refinements, sentiment analysis approaches remain fundamentally limited. They often miss the intricacy inherent in news by focusing on linguistic patterns rather than economically relevant considerations. 

%----------------------------------------------------
\hspace{0.5cm} \textit{Topic modeling.} 
Beyond sentiment analysis, researchers have also explored topic modeling as an alternative approach to categorize text into broader themes. The pioneering work of 
\cite{antweiler2006us}  Antweiler and Frank (2006) 
demonstrated that computational linguistics methods could reveal important patterns in market reactions to news, finding that stock prices do not immediately and consistently reflect news, with effects varying significantly across different types of stories and market conditions. Topic modeling approaches have since been applied across financial research domains. 
\cite{hansen2018transparency}  Hansen et al. (2018) 
used these techniques to analyze Federal Reserve communications, while 
\cite{bybee2021business}  Bybee et al. (2021) 
developed a topic model analyzing over 800,000 Wall Street Journal articles to track news attention to different economic themes. 
\cite{bybee2023narrative}  Bybee et al. (2023) 
further integrated topic modeling with asset pricing models to derive systematic risk factors from news.
%
However, these models are limited in adapting to new and evolving information and lack the specificity needed to assess the precise impact of news on individual firms or sectors. While topic models can identify broad themes, they struggle to capture the changing context of financial news, particularly when new narratives emerge, such as unexpected geopolitical events or technological disruptions.

%----------------------------------------------------
\hspace{0.5cm} \textit{Vector-based models.} 
Vector-based models have emerged as an alternative approach to address the limitations of both sentiment analysis and topic modeling. The foundational models in this domain, Word2Vec and GloVe, established the paradigm of mapping words to continuous vector spaces based on their co-occurrence patterns, enabling mathematical operations on words and capturing semantic relationships.
\cite{hoberg2016text}  Hoberg and Phillips (2016) 
pioneered their application in finance by developing time-varying measures of product similarity from firms' 10-K descriptions, demonstrating how vector representations could capture nuanced competitive relationships that traditional industry classifications miss.
%
The advent of transformer architectures marked a significant advancement, leading to more sophisticated models such as BERT, RoBERTa, or GPT. These models process text through multiple attention layers, generating context-aware embeddings by considering relationships between all words simultaneously. 
\cite{chen2021stock}  Chen (2021) 
demonstrated their superior performance in predicting stock movements following financial news events, while 
\cite{benincasa2022different}  Benincasa et al. (2022) 
leveraged BERT to develop a novel measure of bond ``greenness'', revealing how subtle textual differences in bond documentation translate into measurable price effects.
%
Recent applications have further expanded the scope of these methods. 
\cite{jha2022does}  Jha et al. (2024) 
analyzed finance sentiment across multiple countries and centuries, while 
\cite{zhang2023feel}  Zhang (2023) 
integrated sentiment analysis from GPT and BERT into traditional asset pricing models. 
\cite{gabaix2023asset}  Gabaix et al. (2023) 
introduced \qquote{asset embeddings}, showing how these techniques can uncover latent firm characteristics from investors' holdings data. However, even when fine-tuned with domain-specific training data (e.g: \texttt{FinBERT}), these methods cannot inherently incorporate economic structure, which limits their ability to comprehend the economic implications of news articles.
\end{quote}

\mx
%----------------------------------------------------
Having examined the limitations of current methodological approaches, we now turn to a second critical gap in the literature: there is an insufficient focus on firm-specific analysis in existing research. Many studies examine the impact of news on broader market indices
 such as the S\&P500 or DJIA, rather than on individual firms. 
 %
For example, 
\cite{cutler1988moves}  Cutler et al. (1988) 
and 
\cite{mitchell1994impact}  Mitchell and Mulherin (1994) 
analyzed comprehensive news coverage to understand aggregate market movements, while more recent work has leveraged increasingly sophisticated data sources. 
\cite{bollen2011twitter} Bollen et al. (2011) 
developed novel mood tracking tools for Twitter messages to predict DJIA movements, and 
\cite{garcia2013sentiment} Garcia (2013) 
examined a century of New York Times financial columns to study market-wide returns during recessions. 
\cite{baker2016measuring}, \cite{baker2021triggers} Baker et al. (2016, 2021) 
and 
\cite{manela2017news} Manela and Moreira (2017) 
constructed innovative news-based indices that have enhanced our understanding of market-wide uncertainty and volatility.
While these and other similar studies provide valuable insights into market-wide reactions, they fall short in elucidating how specific firms are affected by news events. Firm-specific impacts are often masked when aggregated at the index level, leading to a loss of critical information about how particular entities are influenced by specific news. 
For example, during the COVID-19 pandemic, market indices masked substantial heterogeneity in firm-level responses
 with some sectors like technology and healthcare experiencing positive returns, while others, such as hospitality, travel, and retail, experiencing significant negative impacts due to widespread lockdowns and reduced consumer spending. 
Such differences are often obscured when focusing solely on market indices. Tools like Named Entity Recognition (NER), which could help identify firms impacted by particular events, remain underutilized in financial research, further contributing to the lack of firm-level granularity.


\mx 
%----------------------------------------------------
The third and final critical issue is the over-reliance on headlines as the basis for news analysis.
Headlines are often used due to their availability and the simplicity of extracting sentiment from them, making them convenient but insufficient for comprehensive analysis. 
\cite{chan2003stock}  Chan (2003) 
provided early evidence of this limitation, showing distinct market reactions to headline news versus no-news events, particularly in terms of drift after bad news and reversals after extreme price movements. As natural language processing techniques evolved, researchers continued to focus primarily on headlines: 
\cite{oncharoen2018deep}  Oncharoen and Vateekul (2018) 
and 
\cite{wei2018stock}  Wei and Nguyen (2020) 
applied increasingly sophisticated deep learning and BERT models to headline analysis, while recent work by 
\cite{lopez2023can}  Lopez-Lira and Tang (2023) 
and 
\cite{chen2022expected}  Chen et al. (2022) 
has extended this approach using large language models to extract contextualized representations from news headlines.
While these studies have advanced our understanding of market reactions to news, headlines are designed to capture attention rather than provide comprehensive information. Consequently, relying solely on headlines can lead to overly simplistic analyses that fail to capture critical contextual details necessary for accurately predicting market reactions.


\mx 
%----------------------------------------------------
This paper seeks to address these three limitations by leveraging Large Language Models (LLMs) to facilitate an economically-structured, granular and firm-specific analysis of complete news articles. 
LLMs are particularly suited for economic interpretation due to their extensive training on human-generated text, including financial and economic discourse. This exposure enables them to ``\textit{understand}'' economic concepts, cause-and-effect relationships, and market mechanisms in ways that mirror human economic reasoning. Unlike purely statistical approaches, LLMs can recognize economic patterns and implications that would be evident to market participants, making them powerful tools for financial analysis.
For example, LLMs could simulate human analysis of news articles, understanding the economic shocks that a news article describes upon a specific firm --such as supply chain disruptions affecting manufacturing, shifts in consumer demand impacting retail, or policy changes influencing energy sectors-- and quantifying both the magnitude and direction of these impacts on specific firms. 
%----------------------------------------------------
In this study, we leverage LLMs to parse a dataset of Spanish business news articles from DowJones Newswires, spanning June 2020 to September 2021, a particularly unstable period marked by economic disruptions due to the COVID-19 pandemic. 
This period was purposefully chosen for its inherent complexity and market instability. Testing our methodology during such a challenging period allows us to rigorously evaluate its robustness and effectiveness. While many methodologies can perform adequately during stable market conditions, their true capabilities are revealed when faced with unprecedented market dynamics and rapid economic changes.

%----------------------------------------------------
\mx
Our methodology consists of defining a schema with which we guide an LLM to detect firm-specific shocks from business news and to further classify them by their type (demand, supply, technological, policy, financial), magnitude (minor, major) and direction (positive, negative). Through their ability to categorize and comprehend the economic implications of news, LLMs generate insights that surpass traditional methodologies, revealing the underlying mechanisms driving market behavior. This allows for a more detailed assessment of how specific pieces of information influence particular firms, providing a richer and more precise picture of market dynamics.
%
%----------------------------------------------------
As our benchmark, we employ a vector-based approach that represents each news article as a high-dimensional embedding vector using a sentence transformer. This benchmark choice serves two key purposes. First, it offers greater granularity and sophistication compared to traditional methods like sentiment analysis and topic modeling. Second, it provides theoretical consistency with our LLM-based approach, as vector embeddings constitute the first layer of an LLM's architecture. This parallel allows us to effectively compare the predictive power of the LLM's initial representation (vector embeddings) with its final output (economically structured news classification). Through this comparison, we can assess whether incorporating economic structure in the LLM processing step enhances our ability to predict market reactions to news.

\mx 
To evaluate the timing ability of our proposed methodology, we develop a trading strategy that builds on the traditional portfolio sorting approach. While conventional strategies sort stocks based on firm characteristics, we instead sort based on news clusters. For the benchmark (vector embeddings), we employ KMeans clustering, while our LLM methodology clusters articles by shock categories. We identify the best and worst-performing clusters by analyzing the stock price responses of affected firms, then construct a long-short portfolio strategy that takes long positions in the best-performing clusters and short positions in the worst-performing ones. The profitability of this strategy serves as a measure of each clustering methodology's ability to identify economically meaningful news patterns that translate into improved market timing abilities.
%
%----------------------------------------------------
Our findings reveal that while the vector-based model successfully identifies firm- and industry-specific clusters, its trading signals lack persistence. The model's reliance on historical firm and industry performance patterns generates ephemeral signals that do not translate well to future market conditions. In contrast, our LLM-based methodology produces clusters based on economically meaningful shock classifications, resulting in more persistent trading signals. The superior out-of-sample performance of our LLM-based trading strategy demonstrates its enhanced capability to capture and interpret market reactions to news, underscoring the advantages of incorporating economic structure into news analysis.

\mx 
%----------------------------------------------------
The objective of this paper is not to parse the largest dataset available or to develop a realistic trading strategy with commercial application. Rather, it aims to introduce a novel methodology for analyzing news articles in a granular and firm-specific manner, demonstrating its utility through a reduced dataset. By focusing on a smaller, high-quality dataset, the study emphasizes methodological rigor and interpretability. The findings are intended to contribute to a more nuanced understanding of how market participants process news, using a simple trading strategy to illustrate the potential of this approach in capturing the complexities of information processing in financial markets. This methodological contribution lays the groundwork for future research that could extend these techniques to larger datasets and more complex trading applications, ultimately enhancing our ability to understand and predict market behavior in response to news.

\mx 
The remainder of this paper is organized as follows: Section 2 presents the dataset and preprocessing steps. Section 3 provides a mathematical framework for analyzing news articles. In Section 4, we focus on clustering news articles -- first presenting the benchmark framework using KMeans clustering of vector embeddings, followed by our novel LLM-based methodology. Section 5 details the construction of a simple trading strategy, including market-beta-neutral positions for each firm-article pair, extraction of cluster-average Sharpe Ratios, and selection of optimal clusters based on two proposed algorithms. In Section 6, we perform robustness checks by examining the sensitivity of our results to hyperparameter variations. Finally, Section 7 concludes and discusses the implications of our findings



%----------------------------------------------------
%----------------------------------------------------



%First, there is a lack of granularity in the analysis of information. Traditional approaches frequently rely on sentiment analysis, reducing the richness of news content to binary classifications of positive or negative sentiment. Despite their reductionist nature, sentiment analysis remains popular due to the ease of implementation and interpretability. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%(\cite{tetlock2007giving}, \cite{tetlock2008more}, \cite{bollen2011twitter}, \cite{hanley2010information}, \cite{loughran2011liability}, \cite{garcia2013sentiment}, \cite{jegadeesh2013word},\cite{wei2018stock}, \cite{ke2019predicting}, \cite{lee2020bert}, 
%\cite{lopez2023can}).
%
%
%\blue{
%The foundational work of Tetlock (2007) established the importance of media sentiment in financial markets, showing that high media pessimism predicts downward pressure on market prices, followed by a reversion to fundamentals. Building on this, Tetlock et al. (2008) demonstrated that the fraction of negative words in firm-specific news stories not only forecasts low firm earnings but also reveals that stock prices briefly underreact to the information embedded in negative words. This finding opened new avenues for exploring how textual analysis could capture hard-to-quantify aspects of firms' fundamentals.
%The methodology for analyzing sentiment has evolved significantly over time. Loughran and McDonald (2011) made a crucial contribution by highlighting that general-purpose dictionaries often misclassify words in financial contexts, leading to the development of specialized financial word lists that better reflect market-relevant tone. Jegadeesh and Wu (2013) further refined this approach by introducing a novel term-weighting methodology, demonstrating that the weighting of terms in content analysis is as crucial as the compilation of word lists themselves.
%The emergence of social media led to new developments in sentiment analysis. Bollen et al. (2011) showed that Twitter mood states could predict changes in DJIA values with significant accuracy, while Garcia (2013) revealed that the predictive power of news sentiment is particularly strong during recessions. More recent studies have leveraged advanced machine learning techniques, with Ke et al. (2019) developing a sophisticated supervised learning framework that constructs sentiment scores specifically adapted to return prediction. The application of transformer-based models has further advanced the field, with Lee et al. (2020) and Wei and Nguyen (2020) demonstrating the effectiveness of BERT-based models in analyzing financial sentiment.
%}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Sentiment analysis often misses the intricacy inherent in news and is based on linguistic patterns, rather than on economically relevant considerations.
% Other studies have sought to enhance this granularity through topic modeling, which categorizes text into broad themes 



%----------------------------------------------------
%----------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%(\cite{antweiler2006us}, \cite{hansen2018transparency}, \cite{bybee2021business}, \cite{bybee2023narrative} among others).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%However, these models are limited in adapting to new and evolving information and lack the specificity needed to assess the precise impact of news on individual firms or sectors. Topic models can identify broad themes, but they struggle to capture the changing context of financial news, particularly when new narratives emerge, such as unexpected geopolitical events or technological disruptions. 


%----------------------------------------------------
%----------------------------------------------------


%Concurrently, other branches of literature experimented with vector-based models 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%(\cite{hoberg2016text}, \cite{chen2021stock}, \cite{jha2022does}, \cite{benincasa2022different}, \cite{zhang2023feel}, \cite{gabaix2023asset}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------
%----------------------------------------------------



%While these studies have significantly advanced our understanding of how news affects markets at the aggregate level, firm-specific impacts are often masked when aggregated at the index level. During the COVID-19 pandemic, for instance, market indices masked substantial heterogeneity in firm-level responses, with some sectors like technology and healthcare experiencing positive returns, while others, such as hospitality, travel, and retail, experiencing significant negative impacts due to widespread lockdowns and reduced consumer spending. Such nuanced differences suggest opportunities for more granular analysis. Tools like Named Entity Recognition (NER), which could help identify firms impacted by particular events, remain underutilized in financial research, further contributing to the lack of firm-level granularity.

%Second, there is an insufficient focus on firm-specific analysis in the existing literature. Many studies examine the impact of news on broader market indices, such as the S\&P500 or DJIA, rather than on individual firms. While research by 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\cite{cutler1988moves}, \cite{mitchell1994impact}, \cite{bollen2011twitter}, \cite{garcia2013sentiment}, \cite{baker2016measuring}, \cite{manela2017news}, \cite{baker2021triggers} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%and others provides valuable insights into market-wide reactions, these studies fall short in elucidating how specific firms are affected by news events. Firm-specific impacts are often masked when aggregated at the index level, leading to a loss of critical information about how particular entities are influenced by specific news. 



%----------------------------------------------------
%----------------------------------------------------



%Third, there is an over-reliance on headlines as the basis for news analysis. Headlines are often used due to their availability and the simplicity of extracting sentiment from them, making them convenient but insufficient for comprehensive analysis. Numerous studies, including 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\cite{chan2003stock}, \cite{oncharoen2018deep}, \cite{wei2018stock}, \cite{lopez2023can}, \cite{chen2022expected} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%utilize headlines to gauge market sentiment.  Headlines are designed to capture attention, not to provide a comprehensive summary of all relevant details. Consequently, relying solely on headlines can lead to overly simplistic analyses that fail to capture critical contextual details necessary for accurately predicting market reactions.



% <In the paragraph below, I revise papers that employ topic modeling. I am trying to smoothly criticize the use of this methodology, so I should not build "hope" or "excitement" about this method here. Hence, rewrite "Recent advances in topic modeling have yielded promising results across different domains of financial research" with something that allows for the smooth transition between sentences, but does not say things like "promising result". Simply objectively talk about the paper, and that's it: don't overemphasize it or don't try to sell it.>
%\red{The pioneering work of Antweiler and Frank (2006) demonstrated that computational linguistics methods could reveal important patterns in market reactions to news, finding that stock prices do not immediately and consistently reflect news, with effects varying significantly across different types of stories and market conditions.
%Recent advances in topic modeling have yielded promising results across different domains of financial research. Hansen et al. (2018) successfully applied these techniques to analyze Federal Reserve communications, demonstrating how topic modeling can extract meaningful economic content from complex institutional discourse. In a comprehensive analysis of business news, Bybee et al. (2021) developed a sophisticated topic model analyzing over 800,000 Wall Street Journal articles, showing how news attention to different themes closely tracks various economic activities and can forecast aggregate stock market returns. Building on this framework, Bybee et al. (2023) further demonstrated how topic modeling can be integrated with asset pricing models to derive interpretable systematic risk factors from news text.}

%----------------------------------------------------
%----------------------------------------------------


%%----------------------------------------------------
%% <The paragraph below is a general exposition of vector-based models. This text is not well connected with the next part (literature review on papers that employ vector-based models). Consider integrating this part with the next (literature review) to improve the cohesion of the text.>
%Concurrently, other branches of literature experimented with vector-based models to address the limitations of both sentiment analysis and topic modeling approaches. Traditional approaches like Word2Vec and GloVe, which map words to continuous vector spaces based on their co-occurrence patterns, revolutionized natural language processing by enabling mathematical operations on words and capturing basic semantic relationships. The advent of transformer architectures marked a significant advancement, leading to more sophisticated models such as \texttt{BERT}, \texttt{RoBERTa} or \texttt{GPT}. These transformer-based models process text through multiple attention layers, allowing them to generate context-aware embeddings by considering the relationships between all words in a sequence simultaneously. 

%%----------------------------------------------------
%% % <In the text below I present a literature review on papers that use vector-based methods. This part is too long, and is not very well connected with the previous paragraph. Consider adjusting the length of this text and also, modifying this and the previous paragraph
%Hoberg and Phillips (2016) pioneered the application of text-based vector analysis in finance by developing time-varying measures of product similarity from firms' 10-K descriptions, demonstrating how vector representations could capture nuanced competitive relationships that traditional industry classifications miss. This work highlighted the potential of vector-based approaches to uncover subtle patterns in financial texts that more traditional methods might overlook. 
%% %
%The emergence of sophisticated language models has further advanced this approach. Chen (2021) demonstrated the superior performance of contextualized embeddings from BERT in predicting stock movements following financial news events, achieving significant improvements over traditional static embedding methods. Similarly, Benincasa et al. (2022) leveraged BERT's natural language processing capabilities to develop a novel measure of bond 'greenness,' revealing how subtle textual differences in bond documentation can translate into measurable price effects.
%% %
%Recent work has pushed the boundaries of vector-based approaches in finance even further. Jha et al. (2024) applied language embeddings to analyze finance sentiment across multiple countries and centuries, showing how these techniques can extract meaningful signals from vast historical datasets. Zhang (2023) integrated sentiment analysis from GPT and BERT into traditional asset pricing models, demonstrating improved explanatory power over standard CAPM specifications. Most recently, Gabaix et al. (2023) introduced the concept of 'asset embeddings,' showing how embedding techniques can uncover latent firm characteristics from investors' holdings data, bridging the gap between artificial intelligence advances and traditional finance applications.
%%
%However, even when fine-tuned with domain-specific training data (e.g., FinBERT), these methods cannot inherently incorporate economic structure, limiting their ability to comprehend the economic implications of news articles.
%}




%----------------------------------------------------
% % <Here, I introduce the paragraph below with "Second", which refers to the 2nd issue that I see in the literature, and that I aim to address in my reserach. However, I think that the text is so long that the reader may have already forgotten that I am trying to expose 3 issues in the literature, and may not understand well what this "Second" refers to.>
%Second, there is an insufficient focus on firm-specific analysis in the existing literature. Many studies examine the impact of news on broader market indices,


%----------------------------------------------------
% <I don't like the expression "beginning with", I think this could be better expressed: we want to convey that these are the pioneering or foundational models. Convey this idea better>
%Vector-based models have emerged as an alternative approach to address the limitations of both sentiment analysis and topic modeling. These models, beginning with Word2Vec and GloVe, map words to continuous vector spaces based on their co-occurrence patterns, enabling mathematical operations on words and capturing semantic relationships.


%----------------------------------------------------
% <Here, is it clear that we are talking about the third critical gap in the literature? or should we clarify the beginning of this paragraph more?>
%\red{Third, there is an over-reliance on headlines as the basis for news analysis. }



%----------------------------------------------------

% PARAGRAPH [A]. In this paragraph I start talking about clustering, and the reader may get lost. This is something that I explain a bit better in PARAGRAPH [B]. As a successful academic in finance, your task is to help me structure the paragraphs in the optimal way so that ideas flow naturally and so that the reader is never lost. So let me know how I can optimally structure my text to make the ideas flow smoothly and to ensure the ideas are presented timely so that the reader doesn't find him/herself lost in the reading.


%As a benchmark, we compare our LLM clustering method with KMeans clustering of embeddings. That is, one could simply represent a news article as a  vector in high-dimensional space and then apply an unsupervised clustering method such as KMeans. 
% This approach is more sophisticated and granular than sentiment- or topic-based methods, but it still lacks an economically meaningful parsing of the news article content.


% KMeans clustering of embeddings. This benchmark represents a more sophisticated approach than sentiment- or topic-based methods, where news articles are represented as vectors in high-dimensional space and then grouped using unsupervised clustering. While this vector-based approach offers improvements over traditional methods, it still lacks an economically meaningful parsing of the news article content. Our LLM-based methodology addresses this limitation by incorporating economic structure directly into the analysis.

%----------------------------------------------------
% % PARAGRAPH [A]. <IMPROVE THE WRITING OF THIS PARAGRAPH: I wrote it fast. Rewrite it better to ensure clarity, cohesion and integration with the rest of the text.>
%To evaluate our approach, we establish a benchmark comparison using a sophisticated vector-based approach. In particular, we represent  each article as a high dimensional embedding vector using a sentence transformer. We choose this benchmark for two reasons: 1) it is the most granular and sophisticated method available 2) it makes theoretical sense, as vector embeddings are the first layer of an LLM. Hence, we are effectively comparing the first layer of the LLM (vector embedding representation of the article) with the last layer (parsed news articles accroding to our shock classification schema). This allows us to evaluate whether incorporating economic structure in the LLM processing step produces superior insights into predicing market reactionsto news. 


%----------------------------------------------------
% <These are 2 writeups that convey the same idea: Option 1 and Option 2. You have two tasks: 1) Tell me which option is better. 2) From the selected best option, modify this: instead of saying that vector embeddings are the best method currently available in the literature, try to be less exaggerated. try to sell it as being the methodology that is more sophisticated and granular than sentiment analysis, topic models and basic vector models. or say it as you wish, but simply avoid making epic statements. In academica, we don't like epic statements.">

% <OPTION 1>
%As our benchmark, we employ a vector-based approach that represents each news article as a high-dimensional embedding vector using a sentence transformer. This benchmark choice serves two key purposes. First, it offers greater granularity and sophistication compared to traditional methods like sentiment analysis and topic modeling. Second, it provides theoretical consistency with our LLM-based approach, as vector embeddings constitute the first layer of an LLM's architecture. This parallel allows us to effectively compare the predictive power of the LLM's initial representation (vector embeddings) with its final output (economically structured news classification). Through this comparison, we can assess whether incorporating economic structure in the LLM processing step enhances our ability to predict market reactions to news.

% <OPTION 2>
%Our empirical strategy involves a benchmark comparison that allows us to isolate the value of incorporating economic structure in news analysis. Specifically, we represent each article as a high-dimensional embedding vector using a sentence transformer. This benchmark is both theoretically and practically relevant: theoretically, as vector embeddings constitute the first layer of an LLM, allowing us to compare the initial representation (embeddings) with the final output (our economic shock classification); and practically, as it offers greater granularity and sophistication compared to traditional sentiment analysis and topic modeling approaches. This design enables us to evaluate whether incorporating economic structure in the LLM processing step produces superior insights for predicting market reactions to news



%----------------------------------------------------
% <HERE, IT SEEMS WEIRD THAT WE ARE SAYING something like "reserachers moved away from sentiment analysis because of their limitations", when we are literally referencing a paper that was done in 2006 (before the first reference we presented on sentiment analysis) and when we presented papers from 2020 and there are still papers being done now with sentiment analysis. Perhaps, we should introduce this section in a different way to avoid the referee thinking that we are stupid.>

%The limitations from sentiment analysis has led researchers to explore alternative approaches, such as topic modeling, which attempts to categorize text into broader themes.