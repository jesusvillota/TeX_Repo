\section{Implementation}

In this section, we describe the technical implementation of the Reinforcement Learning (RL) model for asset pricing. We outline the neural network architecture used for policy optimization, the training procedure, the regularization techniques employed to prevent overfitting, and the hyperparameter tuning process. Additionally, we discuss the integration of Generative Adversarial Networks (GANs) with the RL framework to further enhance the model's ability to capture pricing errors in asset returns.

\subsection{Neural Network Architecture}

The RL model is implemented using a neural network architecture to parameterize both the policy and the value function in the actor-critic framework. The two main components of the architecture are the \textit{actor network} and the \textit{critic network}.

\subsubsection{Actor Network}

The \textit{actor network} represents the policy $\pi(s_t)$, which maps the state $s_t$ to the action $a_t = \{\omega_t, \beta_t\}$. The architecture of the actor network consists of the following layers:
\begin{itemize}
    \item \textbf{Input Layer}: The input to the actor network is the state vector $s_t = \{ I_t, I_{t,i}, \omega_{t-1}, h_t \}$, which includes macroeconomic variables, firm-specific characteristics, previous portfolio weights, and the hidden state $h_t$ from the Long Short-Term Memory (LSTM) network.
    \item \textbf{Hidden Layers}: The hidden layers are composed of fully connected layers with non-linear activation functions. We use the \textit{Rectified Linear Unit (ReLU)} activation function:
    \[
    \text{ReLU}(x) = \max(0, x)
    \]
    The hidden layers allow the network to capture non-linear relationships and interaction effects between macroeconomic variables and asset characteristics.
    \item \textbf{Output Layer}: The output of the actor network is the action $a_t = \{\omega_t, \beta_t\}$, which consists of the portfolio weights and risk loadings. The action space is continuous, and we use a \textit{softmax} function to ensure that the portfolio weights sum to 1:
    \[
    \omega_t = \text{softmax}(W^\top s_t + b)
    \]
    where $W$ and $b$ are the weight matrix and bias term for the output layer.
\end{itemize}

\subsubsection{Critic Network}

The \textit{critic network} estimates the value function $V^\pi(s_t)$, which represents the expected cumulative reward for a given state $s_t$ under policy $\pi$. The architecture of the critic network is similar to the actor network:
\begin{itemize}
    \item \textbf{Input Layer}: The input is the same state vector $s_t = \{ I_t, I_{t,i}, \omega_{t-1}, h_t \}$ used in the actor network.
    \item \textbf{Hidden Layers}: Fully connected layers with ReLU activation are used to model non-linear relationships between state variables and the value function.
    \item \textbf{Output Layer}: The output is the scalar value $V^\pi(s_t)$, representing the expected cumulative reward from state $s_t$. No activation function is applied to the output layer.
\end{itemize}

\subsection{Training Procedure}

The RL model is trained using a gradient-based optimization method to adjust the parameters of the actor and critic networks. We adopt the \textit{Proximal Policy Optimization (PPO)} algorithm, a policy gradient method known for its stability and efficiency in training RL models. The training process consists of the following steps:

\begin{enumerate}
    \item \textbf{Collect Experience}: The agent interacts with the environment (the financial market) by observing the state $s_t$, selecting an action $a_t$, and receiving a reward $R_t$. The experience tuple $(s_t, a_t, R_t, s_{t+1})$ is stored in a replay buffer.
    \item \textbf{Compute Advantage}: The advantage function $A(s_t, a_t)$ is computed as the difference between the actual reward and the expected value from the critic network:
    \[
    A(s_t, a_t) = R_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)
    \]
    where $\gamma \in (0, 1]$ is the discount factor that controls the weight given to future rewards.
    \item \textbf{Policy Update (Actor Network)}: The policy is updated by minimizing the loss function:
    \[
    L_\text{actor}(\theta) = -\mathbb{E}\left[A(s_t, a_t) \log \pi_\theta(a_t \mid s_t)\right]
    \]
    where $\theta$ are the parameters of the actor network.
    \item \textbf{Value Function Update (Critic Network)}: The critic network is updated by minimizing the mean squared error (MSE) between the predicted and actual value function:
    \[
    L_\text{critic}(\phi) = \mathbb{E}\left[\left(R_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)\right)^2\right]
    \]
    where $\phi$ are the parameters of the critic network.
    \item \textbf{Repeat}: The process is repeated over multiple episodes until the policy converges to an optimal strategy.
\end{enumerate}

\subsection{Regularization Techniques}

To prevent overfitting, we apply several regularization techniques during training:
\begin{itemize}
    \item \textbf{Dropout}: Dropout is used in the hidden layers of both the actor and critic networks to prevent overfitting. During training, a fraction of the neurons in each hidden layer is randomly set to zero, which helps the network generalize better.
    \item \textbf{Early Stopping}: We monitor the performance of the model on a validation set during training. If the validation performance stops improving after a certain number of epochs, training is stopped to prevent overfitting.
    \item \textbf{$L_2$ Regularization}: A penalty is added to the loss function based on the $L_2$ norm of the weights. This encourages the model to learn simpler, more generalizable representations.
\end{itemize}

\subsection{Hyperparameter Tuning}

Hyperparameters such as the learning rate, discount factor $\gamma$, and the number of hidden layers and neurons in each layer are tuned using cross-validation. The following hyperparameters are optimized:
\begin{itemize}
    \item \textbf{Learning Rate}: The step size used by the gradient descent algorithm to update the network weights.
    \item \textbf{Discount Factor ($\gamma$)}: Controls the weight given to future rewards. A higher $\gamma$ encourages the model to prioritize long-term rewards.
    \item \textbf{Batch Size}: The number of experience tuples used in each training iteration.
    \item \textbf{Number of Hidden Layers and Neurons}: The depth and width of the actor and critic networks are tuned to capture the complexity of the asset pricing problem.
\end{itemize}

\subsection{GAN Integration}

The Generative Adversarial Network (GAN) from \textit{Chen, Pelger, and Zhu (2023)} is integrated into the RL framework to enhance the model's ability to minimize pricing errors. The GAN consists of two networks:
\begin{itemize}
    \item \textbf{Generator}: The generator network in the GAN generates synthetic data points, which represent the hardest-to-price moments. These moments are selected to maximize pricing errors.
    \item \textbf{Discriminator}: The discriminator network evaluates the pricing performance of the agent's policy by comparing real and synthetic pricing errors.
\end{itemize}

At each time step, the GAN adversary generates a synthetic moment condition designed to maximize pricing discrepancies. The RL agent then adjusts the portfolio weights $\omega_t$ and risk exposures $\beta_t$ to minimize the pricing error in response to these synthetic challenges. This adversarial setup ensures that the RL model learns to handle the most difficult pricing conditions, thereby improving its robustness.
