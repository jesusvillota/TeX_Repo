\section{Literature Review}

The issue of overstated profitability in out-of-sample tests has long been recognized in the finance literature. Two primary mechanisms are responsible for the decay of trading strategy profitability over time: out-of-sample overfitting due to model tuning and the erosion of profitability due to market efficiency. This section reviews the relevant literature on these two topics, laying the foundation for the theoretical and empirical analyses in subsequent sections.

\subsection{Overfitting and Model Tuning}

The concept of overfitting arises when a model is excessively complex relative to the amount of data available, leading to excellent performance on the training data but poor generalization to new, unseen data. This issue is particularly pronounced in machine learning-based trading strategies, where researchers can easily adjust hyperparameters such as the structure of neural networks to maximize performance on a given data set. The risk of overfitting is exacerbated when researchers peek at test data during the model development process, tuning their models to optimize performance on the test set rather than relying on a proper separation between training, validation, and test sets.

Harvey, Liu, and Zhu (2016) provide a comprehensive examination of the dangers of multiple testing in finance, emphasizing that researchers may inadvertently overfit their models by testing numerous strategies and selectively reporting the most profitable ones. They argue that this practice leads to an overstatement of out-of-sample profitability and calls for more rigorous standards in model validation. Similarly, Bianchi, Drew, and Walk (2019) highlight how hyperparameter tuning can inflate performance metrics, leading to misleading conclusions about a model's generalizability.

\subsection{Data Snooping in Financial Models}

Closely related to overfitting is the issue of data snooping, where researchers test a multitude of strategies on the same data set, eventually finding one that appears to perform well by chance alone. Sullivan, Timmermann, and White (1999) provide one of the earliest comprehensive treatments of data-snooping biases in financial research. They show that when many strategies are tested on the same data set, the probability of finding a profitable strategy purely by chance increases dramatically. This issue is compounded by the fact that only the most successful strategies are published, creating a skewed perception of profitability in the academic literature.

To combat data-snooping biases, Harvey and Liu (2019) propose the use of cross-validation techniques commonly employed in machine learning. Cross-validation helps mitigate overfitting by repeatedly splitting the data into training and test sets, providing a more robust estimate of out-of-sample performance. Despite these advancements, the problem persists, particularly in academic settings where the pressure to publish significant results may lead researchers to cut corners in their empirical methodologies.

\subsection{Market Efficiency and Profitability Decay}

The second mechanism responsible for the decline in trading strategy profitability is the well-documented phenomenon of post-publication decay, which is a direct consequence of market efficiency. According to the efficient market hypothesis (EMH) introduced by Fama (1970), any publicly known trading strategy should eventually become unprofitable as market participants incorporate the strategy into their trading behavior. Once a strategy becomes widely adopted, its potential for generating excess returns is quickly arbitraged away, as rational traders act to eliminate any mispricing in the market.

Recent empirical work by McLean and Pontiff (2016) confirms this theoretical prediction by documenting the decay of profitability for published trading strategies. They find that the average alpha of strategies published in academic journals declines significantly after publication, providing strong evidence for the EMH's effect on trading strategy performance. Similarly, Linnainmaa and Roberts (2018) show that the strategies that once exhibited strong profitability eventually see their returns converge to zero as they become more widely known and implemented.

These findings are critical for understanding why the profitability of trading strategies proposed in finance papers tends to be overstated. Whether due to overfitting, data snooping, or market efficiency, the out-of-sample profitability of these strategies is unlikely to be as high as initially reported. This paper builds on this literature by providing a novel empirical approach to testing these strategies over time, using a rolling-window framework to evaluate their performance across different historical periods.
