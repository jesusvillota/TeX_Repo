\documentclass[12pt,article]{memoir}
\usepackage{/Users/jesusvillotamiranda/Documents/LaTeX/$$JVM_Macros}
\Subject{Econometric Explanations}
\Arg{}

\begin{document}
\tableofcontents

\section{Understanding Alpha-Mixing Conditions}

\subsection{Formal Definition and Interpretation}

\subsubsection{Mathematical Setup}

Let $\{X_t\}_{t=-\infty}^{\infty}$ be a stochastic process on a probability space $(\Omega, \mathcal{F}, P)$. We define:

\begin{itemize}
    \item $\mathcal{F}_{-\infty}^t = \sigma(..., X_{t-1}, X_t)$: the $\sigma$-algebra generated by all events up to time $t$
    \item $\mathcal{F}_{t+h}^{\infty} = \sigma(X_{t+h}, X_{t+h+1},...)$: the $\sigma$-algebra generated by all events from time $t+h$ onward
\end{itemize}

\subsubsection{Alpha-Mixing Coefficient}

The $\alpha$-mixing coefficient is defined as:

\begin{equation}
    \alpha(h) = \sup_{A \in \mathcal{F}_{-\infty}^t, B \in \mathcal{F}_{t+h}^{\infty}} |P(A \cap B) - P(A)P(B)|
\end{equation}

\textbf{Interpretation:}
\begin{itemize}
    \item $P(A \cap B)$ is the joint probability of events $A$ and $B$
    \item $P(A)P(B)$ is what the joint probability would be if $A$ and $B$ were independent
    \item $\alpha(h)$ measures the maximum deviation from independence at lag $h$
    \item As $h \to \infty$, $\alpha(h) \to 0$ for mixing processes
\end{itemize}

\subsection{Necessity of Alpha-Mixing}

\subsubsection{Statistical Requirements}

Alpha-mixing is needed for:

1. \textbf{Law of Large Numbers:}
\begin{equation}
    \frac{1}{T}\sum_{t=1}^T X_t \xrightarrow{p} E[X_t]
\end{equation}

2. \textbf{Central Limit Theorem:}
\begin{equation}
    \frac{1}{\sqrt{T}}\sum_{t=1}^T (X_t - E[X_t]) \xrightarrow{d} N(0, \sigma^2)
\end{equation}

3. \textbf{Moment Bounds:}
\begin{equation}
    E|\frac{1}{T}\sum_{t=1}^T X_t - E[X_t]|^p \leq CT^{-p/2}
\end{equation}

\subsection{Understanding the Paper's Mixing Condition}

The condition:
\begin{equation}
    \sum_{h=1}^{\infty} h^2\alpha(h)^{\delta/(2+\delta)} < \infty
\end{equation}

\subsubsection{Component Analysis}

1. \textbf{The Role of $h$:}
   \begin{itemize}
       \item $h$ represents the time lag
       \item $h^2$ ensures rapid decay of dependence
       \item Larger $h$ means events further apart in time
   \end{itemize}

2. \textbf{The Role of $\alpha(h)$:}
   \begin{itemize}
       \item Measures dependence at lag $h$
       \item Must decay faster than $h^{-2}$ for summability
       \item Typical decay: $\alpha(h) \sim h^{-\beta}$ for some $\beta > 2$
   \end{itemize}

3. \textbf{The Role of $\delta$:}
   \begin{itemize}
       \item Controls moment existence
       \item Larger $\delta$ means stronger moment conditions
       \item Typically $\delta = 2$ for financial applications
   \end{itemize}

\subsection{Intuitive Examples of Mixing}

\subsubsection{Financial Market Examples}

1. \textbf{Market Microstructure Effects:}
\begin{equation}
    R_t = \phi R_{t-1} + \epsilon_t, \quad |\phi| < 1
\end{equation}
   \begin{itemize}
       \item Bid-ask bounce creates short-term dependence
       \item Effect dies out exponentially: $\alpha(h) \sim |\phi|^h$
   \end{itemize}

2. \textbf{Volatility Clustering:}
\begin{equation}
    R_t = \sigma_t \epsilon_t, \quad \sigma_t^2 = \omega + \alpha R_{t-1}^2 + \beta \sigma_{t-1}^2
\end{equation}
   \begin{itemize}
       \item GARCH processes are $\alpha$-mixing
       \item Dependence decays geometrically
   \end{itemize}

\subsection{Verifying Mixing Conditions in Practice}

\subsubsection{Statistical Tests}

1. \textbf{Correlation-based Tests:}
\begin{equation}
    \hat{\rho}(h) = \frac{\sum_{t=h+1}^T (X_t - \bar{X})(X_{t-h} - \bar{X})}{\sum_{t=1}^T (X_t - \bar{X})^2}
\end{equation}

2. \textbf{Mixing Coefficient Estimation:}
\begin{equation}
    \hat{\alpha}(h) = \sup_{i,j} |\hat{P}(A_i \cap B_j) - \hat{P}(A_i)\hat{P}(B_j)|
\end{equation}

\subsubsection{Practical Approaches}

1. \textbf{Graphical Analysis:}
   \begin{itemize}
       \item Plot ACF/PACF
       \item Examine decay patterns
       \item Check for long-range dependence
   \end{itemize}

2. \textbf{Model-based Verification:}
   \begin{itemize}
       \item Fit ARMA/GARCH models
       \item Check residual properties
       \item Verify model stability
   \end{itemize}

\subsection{Connection to Other Time Series Concepts}

\subsubsection{Related Dependencies}

1. \textbf{Relationship to Ergodicity:}
\begin{equation}
    \text{$\alpha$-mixing} \implies \text{ergodicity}
\end{equation}

2. \textbf{Comparison with Other Mixing Types:}
   \begin{itemize}
       \item $\beta$-mixing (absolute regularity)
       \item $\phi$-mixing (uniform mixing)
       \item $\rho$-mixing (maximal correlation)
   \end{itemize}

\subsubsection{Hierarchy of Conditions}
\begin{equation}
    \text{i.i.d.} \implies \text{$\phi$-mixing} \implies \text{$\rho$-mixing} \implies \text{$\beta$-mixing} \implies \text{$\alpha$-mixing}
\end{equation}

\subsection{Stock Return Properties and Mixing}

\subsubsection{Empirical Evidence}

1. \textbf{Return Characteristics:}
   \begin{itemize}
       \item Weak serial correlation in returns
       \item Strong dependence in volatility
       \item Leverage effects
   \end{itemize}

2. \textbf{Market Efficiency Implications:}
\begin{equation}
    \alpha(h) \leq Ch^{-\beta}, \quad \beta > 2
\end{equation}
   \begin{itemize}
       \item Consistent with weak-form efficiency
       \item Allows for volatility clustering
       \item Permits predictability in higher moments
   \end{itemize}

%\end{antml:parameter>
%<parameter name="language">latex

\section{Understanding Moment Conditions}

\subsection{Overview of Moment Conditions}

The moment conditions in our assumption require finite $(4+\delta)$-th moments for returns, errors, and factors. Let's understand why each condition is necessary and what it buys us in terms of asymptotic theory.

\subsection{Detailed Analysis of Each Condition}

\subsubsection{Condition (a): $E|R_{it}|^{4+\delta} < \infty$}

This condition on asset returns is needed for several crucial reasons:

1. \textbf{Convergence Rates:}
\begin{equation}
    \sqrt{T}(\hat{w}_T - w_0) \xrightarrow{d} N(0, V)
\end{equation}

The fourth moment ensures:
\begin{itemize}
    \item Existence of the asymptotic variance $V$
    \item Validity of the CLT for sample moments
    \item Uniform convergence of sample covariances
\end{itemize}

2. \textbf{Berry-Esseen Bounds:}
\begin{equation}
    \sup_x |P(\sqrt{T}(\hat{w}_T - w_0) \leq x) - \Phi(x)| \leq \frac{C}{\sqrt{T}}
\end{equation}

The extra $\delta$ moment ($E|R_{it}|^{\delta} < \infty$) provides:
\begin{itemize}
    \item Better convergence rates
    \item Uniform integrability
    \item Tighter finite sample bounds
\end{itemize}

\subsubsection{Condition (b): $E|\epsilon_{it}|^{4+\delta} < \infty$}

This condition on error terms is crucial for:

1. \textbf{Variance Estimation:}
\begin{equation}
    \hat{\Sigma}_T - \Sigma = O_p(T^{-1/2})
\end{equation}

Where:
\begin{itemize}
    \item $\hat{\Sigma}_T$ is the sample variance of errors
    \item Fourth moments ensure consistency of variance estimators
    \item Extra $\delta$ provides uniform convergence
\end{itemize}

2. \textbf{HAC Estimation:}
\begin{equation}
    \|\hat{\Omega}_T - \Omega\|_2 = O_p((T/m_T)^{-1/2} + m_T^{-q})
\end{equation}

Where:
\begin{itemize}
    \item $\hat{\Omega}_T$ is the HAC estimator
    \item Fourth moments ensure kernel estimator convergence
    \item $\delta$ allows for optimal bandwidth selection
\end{itemize}

\subsubsection{Condition (c): $\sup_t E\|F_t\|^{4+\delta} < \infty$}

This condition on factors enables:

1. \textbf{Factor Structure Analysis:}
\begin{equation}
    R_{it} = \beta_i'F_t + \epsilon_{it}
\end{equation}

Providing:
\begin{itemize}
    \item Well-defined factor loadings
    \item Stable estimation procedures
    \item Valid cross-sectional inference
\end{itemize}

2. \textbf{Uniform Bounds:}
\begin{equation}
    \sup_{t,T} E\|\frac{1}{\sqrt{T}}\sum_{s=1}^t (F_sF_s' - E[F_sF_s'])\|_2 < \infty
\end{equation}

\subsection{Technical Implications}

\subsubsection{Why $4+\delta$ Specifically?}

1. \textbf{Fourth Moments:}
   \begin{itemize}
       \item Required for CLT with dependent data
       \item Needed for convergence of sample covariances
       \item Essential for HAC estimation
   \end{itemize}

2. \textbf{The Role of $\delta$:}
   \begin{itemize}
       \item Provides room for Lyapunov condition
       \item Ensures uniform integrability
       \item Allows for stronger convergence rates
   \end{itemize}

\subsection{Practical Considerations}

\subsubsection{Verification in Financial Data}

1. \textbf{Return Distributions:}
\begin{equation}
    \text{Kurtosis} = \frac{E[R_{it}^4]}{(E[R_{it}^2])^2}
\end{equation}

Typical findings:
\begin{itemize}
    \item Daily returns: kurtosis $\approx 5-10$
    \item Weekly returns: kurtosis $\approx 4-6$
    \item Monthly returns: kurtosis $\approx 3-4$
\end{itemize}

2. \textbf{Factor Properties:}
\begin{equation}
    \text{Tail Index} = \lim_{x \to \infty} \frac{\log P(|F_t| > x)}{\log x}
\end{equation}

Common observations:
\begin{itemize}
    \item Market factor: tail index $\approx 4-5$
    \item Size factor: tail index $\approx 3-4$
    \item Value factor: tail index $\approx 4-5$
\end{itemize}

\subsection{Consequences of Violation}

If moment conditions fail:

1. \textbf{Statistical Issues:}
   \begin{itemize}
       \item Inconsistent variance estimation
       \item Invalid confidence intervals
       \item Poor finite sample properties
   \end{itemize}

2. \textbf{Econometric Problems:}
   \begin{itemize}
       \item Unstable parameter estimates
       \item Unreliable hypothesis tests
       \item Invalid bootstrap procedures
   \end{itemize}

%\end{antml:parameter>
%<parameter name="language">latex

\section{Understanding Weight Convergence}

\subsection{Basic Concepts of Convergence}

\subsubsection{What is Convergence?}

In our context, convergence means that our estimated weights ($w_T^*$) get arbitrarily close to the true weights ($w^0$) as our sample size ($T$) increases:

\begin{equation}
    \|w_T^* - w^0\| \xrightarrow{p} 0
\end{equation}

This means:
\begin{itemize}
    \item For any small error $\epsilon > 0$
    \item The probability of being more than $\epsilon$ away from $w^0$
    \item Goes to zero as $T \to \infty$
\end{itemize}

\subsection{Why Do We Need Assumptions 1-3?}

\subsubsection{Assumption 1: Data Generating Process}
\begin{equation}
    R_{it} = \mu_i(F_t) + \epsilon_{it}
\end{equation}

This assumption is needed because:
\begin{itemize}
    \item Ensures returns have a factor structure
    \item Guarantees existence of synthetic portfolios
    \item Provides structure for identification
\end{itemize}

\subsubsection{Assumption 2: Mixing Conditions}
\begin{equation}
    \sum_{h=1}^{\infty} h^2\alpha(h)^{\delta/(2+\delta)} < \infty
\end{equation}

This is crucial because:
\begin{itemize}
    \item Allows for dependent data
    \item Ensures sample averages converge
    \item Permits use of uniform LLN
\end{itemize}

\subsubsection{Assumption 3: Moment Conditions}
\begin{equation}
    E|R_{it}|^{4+\delta} < \infty
\end{equation}

Required for:
\begin{itemize}
    \item Existence of limiting distributions
    \item Uniform convergence of sample moments
    \item Well-behaved asymptotic theory
\end{itemize}

\subsection{Understanding Uniform Convergence}

\subsubsection{What is Uniform Convergence?}

For functions $f_n, f$ on space $\mathcal{W}$:
\begin{equation}
    \sup_{w \in \mathcal{W}} |f_n(w) - f(w)| \xrightarrow{p} 0
\end{equation}

Key aspects:
\begin{itemize}
    \item Convergence happens simultaneously for all $w$
    \item Rate of convergence is uniform across $\mathcal{W}$
    \item Stronger than pointwise convergence
\end{itemize}

\subsubsection{Why Do We Need Uniform Convergence?}

Critical because:
\begin{itemize}
    \item Ensures consistency of extremum estimators
    \item Prevents convergence from failing at the optimum
    \item Allows interchange of limits and optimization
\end{itemize}

\subsection{The Uniform Law of Large Numbers (ULLN)}

\subsubsection{What is ULLN?}

For a sequence of functions $\{g_t(w)\}$:
\begin{equation}
    \sup_{w \in \mathcal{W}} |\frac{1}{T}\sum_{t=1}^T g_t(w) - E[g_t(w)]| \xrightarrow{p} 0
\end{equation}

Why we need it:
\begin{itemize}
    \item Ensures objective function converges uniformly
    \item Provides rate of convergence
    \item Handles dependent data through mixing
\end{itemize}

\subsection{The Second Moment Return Matrix}

\subsubsection{Definition}

The second moment return matrix $\Sigma$ is:
\begin{equation}
    \Sigma = E[R_tR_t']
\end{equation}

where $R_t = (R_{1t}, ..., R_{Jt})'$

\subsubsection{Positive Definiteness}

A matrix $\Sigma$ is positive definite if:
\begin{equation}
    x'\Sigma x > 0 \quad \text{for all } x \neq 0
\end{equation}

Why it matters:
\begin{itemize}
    \item Ensures unique solution exists
    \item Guarantees identification
    \item Provides stability for estimation
\end{itemize}

\subsection{Establishing Identification}

\subsubsection{What is Identification?}

Identification means:
\begin{equation}
    w^0 = \arg\min_{w \in \mathcal{W}} Q(w) \quad \text{uniquely}
\end{equation}

Where:
\begin{itemize}
    \item $Q(w)$ is the population objective function
    \item $w^0$ is the unique minimizer
    \item No other weights give same synthetic returns
\end{itemize}

\subsubsection{Role of Positive Definiteness}

The objective function can be written as:
\begin{equation}
    Q(w) = (w - w^0)'\Sigma(w - w^0)
\end{equation}

Positive definiteness ensures:
\begin{itemize}
    \item $Q(w) > 0$ for all $w \neq w^0$
    \item $Q(w^0) = 0$
    \item Unique minimum at $w^0$
\end{itemize}

\subsection{Why is the Return Matrix Positive Definite?}

\subsubsection{Economic Arguments}

1. \textbf{No Arbitrage:}
\begin{itemize}
    \item Perfect correlation implies arbitrage
    \item Markets eliminate arbitrage
    \item Therefore, returns can't be perfectly correlated
\end{itemize}

2. \textbf{Diversification:}
\begin{itemize}
    \item Assets have unique risk components
    \item Not all risk can be diversified away
    \item Implies linear independence of returns
\end{itemize}

\subsubsection{Statistical Verification}

We can verify positive definiteness by:
\begin{equation}
    \lambda_{min}(\hat{\Sigma}) > 0
\end{equation}

Where:
\begin{itemize}
    \item $\lambda_{min}$ is the smallest eigenvalue
    \item $\hat{\Sigma}$ is the sample covariance
    \item Test statistic follows chi-square distribution
\end{itemize}

\subsection{Full Proof Structure}

1. \textbf{Show Uniform Convergence:}
\begin{equation}
    \sup_{w \in \mathcal{W}} |Q_T(w) - Q(w)| \xrightarrow{p} 0
\end{equation}

2. \textbf{Apply ULLN:}
\begin{equation}
    \|Q_T(w) - Q(w)\|_{\infty} = O_p(T^{-1/2}\log T)
\end{equation}

3. \textbf{Use Identification:}
\begin{itemize}
    \item Positive definiteness ensures unique minimum
    \item ULLN ensures sample objective converges
    \item Therefore, minimizer converges to $w^0$
\end{itemize}

4. \textbf{Conclude:}
\begin{equation}
    \|w_T^* - w^0\| \xrightarrow{p} 0
\end{equation}

%\end{antml:parameter>
%<parameter name="language">latex

\Vhrulefill

\section{Detailed Proof of Weight Consistency}

\subsection{Why We Need Objective Function Convergence}

The logic follows these steps:

1. Our estimator is defined as:
\begin{equation}
    w_T^* = \argmin_{w \in \mathcal{W}} Q_T(w)
\end{equation}

2. The population optimum is:
\begin{equation}
    w^0 = \argmin_{w \in \mathcal{W}} Q(w)
\end{equation}

3. For consistency ($w_T^* \xrightarrow{p} w^0$), we need:
\begin{equation}
    \|Q_T(w) - Q(w)\| \text{ small } \implies \|w_T^* - w^0\| \text{ small}
\end{equation}

This implication requires:
\begin{itemize}
    \item Uniform convergence of $Q_T$ to $Q$
    \item Unique identification of $w^0$
    \item Continuous mapping from objective to weights
\end{itemize}

\subsection{Mathematical Proof of Uniform Convergence}

\subsubsection{Step 1: Express the Objective Functions}

Sample objective:
\begin{equation}
    Q_T(w) = \frac{1}{T}\sum_{t=1}^T (R_{it} - \sum_{j=1}^J w_jR_{jt})^2
\end{equation}

Population objective:
\begin{equation}
    Q(w) = E[(R_{it} - \sum_{j=1}^J w_jR_{jt})^2]
\end{equation}

\subsubsection{Step 2: Decomposition}

Expand the difference:
\begin{align}
    Q_T(w) - Q(w) &= \frac{1}{T}\sum_{t=1}^T (R_{it} - w'R_{t})^2 - E[(R_{it} - w'R_{t})^2] \\
    &= \frac{1}{T}\sum_{t=1}^T (R_{it}^2 - E[R_{it}^2]) \\
    &\quad - 2w'\left(\frac{1}{T}\sum_{t=1}^T R_{t}R_{it} - E[R_{t}R_{it}]\right) \\
    &\quad + w'\left(\frac{1}{T}\sum_{t=1}^T R_{t}R_{t}' - E[R_{t}R_{t}']\right)w
\end{align}

\subsubsection{Step 3: Bound the Supremum}

Using triangle inequality:
\begin{align}
    \sup_{w \in \mathcal{W}} |Q_T(w) - Q(w)| &\leq |\frac{1}{T}\sum_{t=1}^T (R_{it}^2 - E[R_{it}^2])| \\
    &\quad + 2\|w\| \|\frac{1}{T}\sum_{t=1}^T R_{t}R_{it} - E[R_{t}R_{it}]\| \\
    &\quad + \|w\|^2 \|\frac{1}{T}\sum_{t=1}^T R_{t}R_{t}' - E[R_{t}R_{t}']\|
\end{align}

\subsection{Why We Need ULLN and What It Buys Us}

\subsubsection{Role of ULLN}

The ULLN gives us:
\begin{equation}
    \|\frac{1}{T}\sum_{t=1}^T g_t(w) - E[g_t(w)]\|_{\infty} = O_p(T^{-1/2}\log T)
\end{equation}

This provides:
\begin{itemize}
    \item Rate of convergence
    \item Uniform control over $w$
    \item Valid under mixing conditions
\end{itemize}

\subsubsection{Application to Our Setting}

For our components:
\begin{align}
    \|\frac{1}{T}\sum_{t=1}^T R_{t}R_{t}' - E[R_{t}R_{t}']\| &= O_p(T^{-1/2}\log T) \\
    \|\frac{1}{T}\sum_{t=1}^T R_{t}R_{it} - E[R_{t}R_{it}]\| &= O_p(T^{-1/2}\log T) \\
    |\frac{1}{T}\sum_{t=1}^T (R_{it}^2 - E[R_{it}^2])| &= O_p(T^{-1/2}\log T)
\end{align}

\subsection{Complete Proof of Identification}

\subsubsection{Step 1: Express Second-Order Condition}

The population objective can be written as:
\begin{equation}
    Q(w) = E[R_{it}^2] - 2w'E[R_{t}R_{it}] + w'E[R_{t}R_{t}']w
\end{equation}

\subsubsection{Step 2: First-Order Conditions}

Differentiate with respect to $w$:
\begin{equation}
    \nabla Q(w) = -2E[R_{t}R_{it}] + 2E[R_{t}R_{t}']w = 0
\end{equation}

Solving for $w^0$:
\begin{equation}
    w^0 = E[R_{t}R_{t}']^{-1}E[R_{t}R_{it}]
\end{equation}

\subsubsection{Step 3: Verify Second-Order Conditions}

The Hessian is:
\begin{equation}
    \nabla^2 Q(w) = 2E[R_{t}R_{t}'] = 2\Sigma
\end{equation}

Positive definiteness follows because:
\begin{enumerate}
    \item For any $x \neq 0$:
    \begin{equation}
        x'\Sigma x = E[(x'R_t)^2] > 0
    \end{equation}

    \item This holds because:
    \begin{itemize}
        \item No perfect collinearity (by no-arbitrage)
        \item Finite second moments (by assumption)
        \item Non-degenerate returns (by market efficiency)
    \end{itemize}
\end{enumerate}

\subsubsection{Step 4: Complete the Proof}

1. By ULLN:
\begin{equation}
    \sup_{w \in \mathcal{W}} |Q_T(w) - Q(w)| \xrightarrow{p} 0
\end{equation}

2. By positive definiteness:
\begin{equation}
    Q(w) - Q(w^0) \geq \lambda_{min}(\Sigma)\|w - w^0\|^2
\end{equation}

3. Therefore:
\begin{equation}
    \|w_T^* - w^0\| \leq \frac{1}{\lambda_{min}(\Sigma)}\sup_{w \in \mathcal{W}} |Q_T(w) - Q(w)| \xrightarrow{p} 0
\end{equation}

This completes the proof by showing:
\begin{itemize}
    \item Uniform convergence of objective function
    \item Unique identification through positive definiteness
    \item Explicit rate of convergence via ULLN
    \item Direct link between objective and parameter convergence
\end{itemize}

%\end{antml:parameter>
%<parameter name="language">latex


\section{Understanding the Uniform Law of Large Numbers}

\subsection{Origin of the Rate}

\subsubsection{The Standard Result}

The rate $O_p(T^{-1/2}\log T)$ is not standard for i.i.d. data. For i.i.d. observations, we typically have:

\begin{equation}
    \|\frac{1}{T}\sum_{t=1}^T g_t(w) - E[g_t(w)]\|_{\infty} = O_p(T^{-1/2})
\end{equation}

The additional $\log T$ term appears due to:
\begin{itemize}
    \item Dependence in the data (mixing conditions)
    \item Uniformity over the parameter space
    \item Need for maximal inequalities
\end{itemize}

\subsection{Deriving the Rate}

\subsubsection{Key Steps}

1. \textbf{Decomposition:}
For fixed $w$:
\begin{equation}
    \frac{1}{T}\sum_{t=1}^T g_t(w) - E[g_t(w)] = \frac{1}{T}\sum_{t=1}^T [g_t(w) - E[g_t(w)]] \equiv \mathbb{G}_T(w)
\end{equation}

2. \textbf{Covering Numbers:}
Define $\mathcal{N}(\epsilon, \mathcal{W}, \|\cdot\|)$ as the minimum number of $\epsilon$-balls needed to cover $\mathcal{W}$.

3. \textbf{Entropy Condition:}
For some $C < \infty$:
\begin{equation}
    \int_0^1 \sqrt{\log \mathcal{N}(\epsilon, \mathcal{W}, \|\cdot\|)}d\epsilon \leq C
\end{equation}

\subsubsection{Maximal Inequality}

Under mixing conditions, we have:
\begin{equation}
    E[\sup_{w \in \mathcal{W}} |\mathbb{G}_T(w)|] \leq C\left(\frac{\log T}{T}\right)^{1/2}
\end{equation}

This follows from:
\begin{itemize}
    \item Moment bounds from mixing conditions
    \item Entropy integral bound
    \item Chaining argument
\end{itemize}

\subsection{Components of the Rate}

\subsubsection{The $T^{-1/2}$ Term}

This comes from:
\begin{equation}
    \text{Var}\left(\frac{1}{T}\sum_{t=1}^T g_t(w)\right) = O(T^{-1})
\end{equation}

Under mixing:
\begin{equation}
    \sum_{h=1}^{\infty} |\text{Cov}(g_t(w), g_{t+h}(w))| < \infty
\end{equation}

\subsubsection{The $\log T$ Term}

Appears due to:
\begin{equation}
    \sup_{w \in \mathcal{W}} |\mathbb{G}_T(w)| = \max_{1 \leq j \leq N_T} |\mathbb{G}_T(w_j)| + O_p(T^{-1/2})
\end{equation}

Where:
\begin{itemize}
    \item $N_T$ is the covering number
    \item Grows polynomially with $T$
    \item Introduces $\log T$ term
\end{itemize}

\subsection{Uniform Control over $w$}

\subsubsection{Why Uniformity Matters}

The result provides:
\begin{equation}
    P\left(\sup_{w \in \mathcal{W}} |\mathbb{G}_T(w)| > M\left(\frac{\log T}{T}\right)^{1/2}\right) \to 0
\end{equation}

This means:
\begin{itemize}
    \item Control over entire parameter space
    \item Valid for optimization problems
    \item Handles parameter estimation
\end{itemize}

\subsection{Validity Under Mixing}

\subsubsection{Required Conditions}

1. \textbf{Mixing Rate:}
\begin{equation}
    \alpha(h) \leq Ch^{-\beta}, \quad \beta > 2
\end{equation}

2. \textbf{Moment Bounds:}
\begin{equation}
    E|g_t(w)|^{2+\delta} < \infty
\end{equation}

3. \textbf{Lipschitz Condition:}
\begin{equation}
    |g_t(w_1) - g_t(w_2)| \leq L_t\|w_1 - w_2\|
\end{equation}
where $E[L_t^{2+\delta}] < \infty$

\subsection{Technical Extensions}

\subsubsection{Stronger Rates}

Under additional conditions:
\begin{equation}
    \|\mathbb{G}_T\|_{\infty} = O_p\left(\left(\frac{\log\log T}{T}\right)^{1/2}\right)
\end{equation}

Requires:
\begin{itemize}
    \item Stronger mixing ($\beta > 4$)
    \item Higher moments ($4 + \delta$)
    \item Bounded parameter space
\end{itemize}

\subsubsection{Empirical Process Theory}

Connection to:
\begin{equation}
    \{\mathbb{G}_T(w): w \in \mathcal{W}\} \Rightarrow \{\mathbb{G}(w): w \in \mathcal{W}\}
\end{equation}

Where:
\begin{itemize}
    \item $\Rightarrow$ denotes weak convergence
    \item $\mathbb{G}$ is a Gaussian process
    \item With covariance kernel from mixing
\end{itemize}

\subsection{Practical Implications}

\subsubsection{For Synthetic Controls}

1. \textbf{Weight Estimation:}
\begin{equation}
    \|w_T^* - w^0\| = O_p\left(\left(\frac{\log T}{T}\right)^{1/2}\right)
\end{equation}

2. \textbf{Inference:}
\begin{equation}
    P(w^0 \in \mathcal{C}_T) = 1 - \alpha + o(1)
\end{equation}

where $\mathcal{C}_T$ is a confidence region constructed using the rate.

%\end{antml:parameter>
%<parameter name="language">latex

\subsection{Machine Learning SCM [CLAUDE]} 
We can extend the linear SCM framework to capture nonlinear relationships between financial instruments using machine learning methods. Let $f_{\theta}:\R^J\to\R$ be a neural network parameterized by $\theta$ that maps the returns of the donor pool to a synthetic return:

$$
R_{0t}^* = f_{\theta}(R_{1t},\ldots,R_{Jt})
$$

The network parameters $\theta$ are trained to minimize the loss function:
$$
\mathcal{L}(\theta) = \frac{1}{T_{tr}}\sum_{t\in\mathcal T_{tr}} (R_{0t} - f_{\theta}(R_{1t},\ldots,R_{Jt}))^2 + \lambda \mathcal{R}(\theta)
$$
where $\mathcal{R}(\theta)$ is a regularization term on the network parameters.

\subsubsection{Architecture Design}
We propose a feed-forward neural network with the following structure:

\begin{itemize}
   \item \textbf{Input Layer}: $J$ nodes corresponding to the donor pool returns
   \item \textbf{Hidden Layers}: Multiple layers with ReLU activation functions
   $$h^{(l+1)} = \text{ReLU}(W^{(l)}h^{(l)} + b^{(l)})$$
   where $W^{(l)}$ and $b^{(l)}$ are the weights and biases of layer $l$
   \item \textbf{Output Layer}: Single node with linear activation to predict the target return
   \item \textbf{Residual Connections}: To facilitate learning of linear relationships, we add skip connections from input to output:
   $$f_{\theta}(x) = \text{NN}_{\theta}(x) + w'x$$
   where $w$ is a learnable weight vector constrained to sum to 1
\end{itemize}

\subsubsection{Training Procedure}
The model is trained using:
\begin{itemize}
   \item \textbf{Loss Function}: Mean squared error with L2 regularization
   $$\mathcal{R}(\theta) = \sum_{l} (\|W^{(l)}\|_F^2 + \|b^{(l)}\|_2^2)$$
   where $\|\cdot\|_F$ denotes the Frobenius norm
   
   \item \textbf{Optimization}: Adam optimizer with learning rate scheduling
   $$\theta_{t+1} = \theta_t - \eta_t \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
   where $\hat{m}_t$ and $\hat{v}_t$ are bias-corrected moment estimates
   
   \item \textbf{Early Stopping}: Training is stopped when validation loss stops improving to prevent overfitting
\end{itemize}

\subsubsection{Ensemble Methods}
To improve robustness, we can employ ensemble methods:

\begin{itemize}
   \item \textbf{Bagging}: Train multiple networks on bootstrap samples of the training data
   $$R_{0t}^* = \frac{1}{K}\sum_{k=1}^K f_{\theta_k}(R_{1t},\ldots,R_{Jt})$$
   where $K$ is the number of networks in the ensemble
   
   \item \textbf{Dropout}: Apply dropout during training and use Monte Carlo dropout during inference
   $$R_{0t}^* = \mathbb{E}_{p(z)}[f_{\theta}(R_{1t},\ldots,R_{Jt},z)]$$
   where $z$ represents random dropout masks
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{landscape}



\begin{table}[H]
 \caption{Statistics of $\mathcal{P}$ Across \protect\linebreak Data Splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\hline
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe} & \textbf{Sortino} & \textbf{Max. DD} & \textbf{Calmar} & \textbf{Skew.} & \textbf{Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\hline 
\multirow{2}{*}{All} & \textit{Greedy} & 1.070 & 5.3 & 9.7 & 0.5 & 0.6 & -6.9 & 0.8 & -0.50 & 4.17 & -0.009 & -0.014 \\  & \textit{Stable} & 1.489 & 35.8 & 16.8 & 1.8 & 2.2 & -7.6 & 4.7 & 0.08 & 5.09 & -0.014 & -0.023 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 0.969 & -4.6 & 11.6 & -0.4 & -0.4 & -6.5 & -0.7 & -0.59 & 2.96 & -0.011 & -0.018 \\  & \textit{Stable} & 1.285 & 46.3 & 19.3 & 2.0 & 2.4 & -7.6 & 6.1 & -0.30 & 3.63 & -0.018 & -0.026 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.088 & 26.6 & 7.3 & 3.2 & 3.7 & -3.5 & 7.7 & -0.49 & 1.19 & -0.006 & -0.010 \\  & \textit{Stable} & 1.149 & 47.7 & 13.3 & 2.9 & 3.4 & -3.6 & 13.1 & -0.24 & 1.78 & -0.012 & -0.018 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.014 & 4.9 & 6.9 & 0.7 & 1.0 & -3.6 & 1.4 & 1.82 & 5.39 & -0.005 & -0.006 \\  & \textit{Stable} & 1.008 & 2.9 & 14.3 & 0.2 & 0.3 & -4.6 & 0.6 & 2.32 & 13.73 & -0.012 & -0.017 \\  \hline 
\end{tabular}
\label{tab:KMeans_portfolio_statistics}

\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\small\textit{Note: The holding period of the beta-neutral strategies is set to $L = 4$ trading days and the number of traded clusters is $\theta = 0.5k = 13$ (as we have $k^* = 26$ clusters). The selection criteria for these parameters is based on maximizing the Sharpe Ratios of the train and validation samples.}
\end{minipage}

\end{table}



\begin{table}[H]
 \caption{(Annualized) Statistics of $\mathcal{P}$ Across Data Splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\hline
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe Ratio} & \textbf{Sortino Ratio} & \textbf{Max. DD} & \textbf{Calmar Ratio} & \textbf{Skew.} & \textbf{Exc. Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\hline 
\multirow{2}{*}{All} & \textit{Greedy} & 1.070 & 5.3 & 9.7 & 0.5 & 0.6 & -6.9 & 0.8 & -0.50 & 4.17 & -13.7 & -22.9 \\  & \textit{Stable} & 1.489 & 35.8 & 16.8 & 1.8 & 2.2 & -7.6 & 4.7 & 0.08 & 5.09 & -22.6 & -36.1 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 0.969 & -4.6 & 11.6 & -0.4 & -0.4 & -6.5 & -0.7 & -0.59 & 2.96 & -18.2 & -28.5 \\  & \textit{Stable} & 1.285 & 46.3 & 19.3 & 2.0 & 2.4 & -7.6 & 6.1 & -0.30 & 3.63 & -28.9 & -41.6 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.088 & 26.6 & 7.3 & 3.2 & 3.7 & -3.5 & 7.7 & -0.49 & 1.19 & -9.5 & -15.9 \\  & \textit{Stable} & 1.149 & 47.7 & 13.3 & 2.9 & 3.4 & -3.6 & 13.1 & -0.24 & 1.78 & -18.3 & -28.1 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.014 & 4.9 & 6.9 & 0.7 & 1.0 & -3.6 & 1.4 & 1.82 & 5.39 & -7.8 & -9.7 \\  & \textit{Stable} & 1.008 & 2.9 & 14.3 & 0.2 & 0.3 & -4.6 & 0.6 & 2.32 & 13.73 & -18.9 & -26.8 \\  \hline 
\end{tabular}
\label{tab:KMeans_portfolio_statistics}

\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\small\textit{Note: The holding period of the beta-neutral strategies is set to $L = 4$ trading days and the number of traded clusters is $\theta = 0.5k = 13$ (as we have $k^* = 26$ clusters). The selection criteria for these parameters is based on maximizing the Sharpe Ratios of the train and validation samples.}
\end{minipage}

\end{table}



\begin{table}[H]
 \caption{Annualized statistics of $\mathcal{P}_{\text{KMeans}}$ Across Data Splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\Xhline{2\arrayrulewidth}
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe Ratio} & \textbf{Sortino Ratio} & \textbf{Max. DD} & \textbf{Calmar Ratio} & \textbf{Skew.} & \textbf{Exc. Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{All} & \textit{Greedy} & 1.070 & 5.3 & 9.7 & 0.5 & 0.6 & -6.9 & 0.8 & -0.45 & 4.04 & -13.7 & -22.9 \\  & \textit{Stable} & 1.489 & 35.8 & 16.8 & 1.8 & 2.2 & -7.6 & 4.7 & 0.19 & 5.08 & -22.6 & -36.1 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 0.969 & -4.6 & 11.6 & -0.4 & -0.4 & -6.5 & -0.7 & -0.54 & 2.86 & -18.2 & -28.5 \\  & \textit{Stable} & 1.285 & 46.3 & 19.3 & 2.0 & 2.4 & -7.6 & 6.1 & -0.20 & 3.49 & -28.9 & -41.6 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.088 & 26.6 & 7.3 & 3.2 & 3.7 & -3.5 & 7.7 & -0.47 & 1.16 & -9.5 & -15.9 \\  & \textit{Stable} & 1.149 & 47.7 & 13.3 & 2.9 & 3.4 & -3.6 & 13.1 & -0.20 & 1.76 & -18.3 & -28.1 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.014 & 4.9 & 6.9 & 0.7 & 1.0 & -3.6 & 1.4 & 1.85 & 5.50 & -7.8 & -9.7 \\  & \textit{Stable} & 1.008 & 2.9 & 14.3 & 0.2 & 0.3 & -4.6 & 0.6 & 2.46 & 14.57 & -18.9 & -26.8 \\  \hline 
\end{tabular}
\label{tab:KMeans_portfolio_statistics}

\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\small\textit{Note: 
Annualized portfolio statistics of the trading strategy based on clusters obtained from applying KMeans to article embeddings. The statistics provided include performance metrics (Cumulative Return, Average Return), risk measures (Standard Deviation, Maximum Drawdown, Value at Risk, Conditional Value at Risk), risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics (Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable. The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-average-$SR$ in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs (shorts) clusters that minimize the rank difference between the training and validation rankings of the cluster-average-$SR$'s subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both algorithms, the cardinality of each leg is upper-bounded by a hyperparameter $\theta$. 
The holding period of the beta-neutral positions is set to $L$ = 4 trading days and the number of traded clusters is $\theta = 0.5k=13$ as there are $k^*=26$ KMeans clusters of article embeddings. The selection criteria for these hyperparameters ($L,\theta$) is based on maximizing the Sharpe Ratios of the train and validation samples.
}
\end{minipage}

\end{table}



\begin{table}[H]
 \caption{Annualized statistics of $\mathcal{P}_{\text{KMeans}}$ across data splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\Xhline{2\arrayrulewidth}
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe Ratio} & \textbf{Sortino Ratio} & \textbf{Max. DD} & \textbf{Calmar Ratio} & \textbf{Skew.} & \textbf{Exc. Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{All} & \textit{Greedy} & 1.310 & 23.1 & 9.6 & 2.2 & 2.9 & -6.3 & 3.7 & 1.47 & 9.93 & -13.6 & -18.9 \\  & \textit{Stable} & 1.365 & 27.0 & 8.6 & 2.8 & 3.4 & -5.9 & 4.6 & 0.28 & 2.24 & -11.9 & -16.9 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 1.112 & 17.6 & 11.4 & 1.4 & 1.9 & -6.3 & 2.8 & 1.65 & 9.00 & -15.7 & -21.0 \\  & \textit{Stable} & 1.177 & 28.3 & 9.9 & 2.5 & 3.0 & -5.9 & 4.8 & 0.16 & 1.71 & -13.5 & -19.6 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.086 & 26.4 & 8.2 & 2.9 & 3.8 & -3.1 & 8.6 & 0.17 & 1.38 & -10.6 & -16.8 \\  & \textit{Stable} & 1.054 & 16.0 & 6.9 & 2.2 & 2.4 & -1.9 & 8.4 & 0.30 & 1.58 & -9.3 & -14.2 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.084 & 30.8 & 6.2 & 4.3 & 6.0 & -1.5 & 21.0 & 1.49 & 8.30 & -6.9 & -9.9 \\  & \textit{Stable} & 1.100 & 37.2 & 7.1 & 4.4 & 7.2 & -1.1 & 34.5 & 0.84 & 1.95 & -9.5 & -11.3 \\  \hline 
\end{tabular}
\label{tab:KMeans_portfolio_statistics}

\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\small\textit{Note: 
Annualized portfolio statistics of the trading strategy based on clusters obtained from applying KMeans to article embeddings. The statistics provided include performance metrics (Cumulative Return, Average Return), risk measures (Standard Deviation, Maximum Drawdown, Value at Risk, Conditional Value at Risk), risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics (Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable. The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-average-$SR$ in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs (shorts) clusters that minimize the rank difference between the training and validation rankings of the cluster-average-$SR$'s subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both algorithms, the cardinality of each leg is upper-bounded by a hyperparameter $\theta$. 
The holding period of the beta-neutral positions is set to $L$ = 4 trading days and the number of traded clusters is $\theta = 0.5k=13$ as there are $k^*=26$ KMeans clusters of article embeddings. The selection criteria for these hyperparameters ($L,\theta$) is based on maximizing the Sharpe Ratios of the train and validation samples.
}
\end{minipage}

\end{table}

\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{landscape}
\begin{table}[H]
 \caption{Statistics of $\mathcal{P}_{\text{LLM}}$ across data splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\Xhline{2\arrayrulewidth}
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe Ratio} & \textbf{Sortino Ratio} & \textbf{Max. DD} & \textbf{Calmar Ratio} & \textbf{Skew.} & \textbf{Exc. Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{All} & \textit{Greedy} & 1.310 & 23.1 & 9.6 & 2.2 & 2.9 & -6.3 & 3.7 & 1.47 & 9.93 & -13.6 & -18.9 \\  & \textit{Stable} & 1.365 & 27.0 & 8.6 & 2.8 & 3.4 & -5.9 & 4.6 & 0.28 & 2.24 & -11.9 & -16.9 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 1.112 & 17.6 & 11.4 & 1.4 & 1.9 & -6.3 & 2.8 & 1.65 & 9.00 & -15.7 & -21.0 \\  & \textit{Stable} & 1.177 & 28.3 & 9.9 & 2.5 & 3.0 & -5.9 & 4.8 & 0.16 & 1.71 & -13.5 & -19.6 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.086 & 26.4 & 8.2 & 2.9 & 3.8 & -3.1 & 8.6 & 0.17 & 1.38 & -10.6 & -16.8 \\  & \textit{Stable} & 1.054 & 16.0 & 6.9 & 2.2 & 2.4 & -1.9 & 8.4 & 0.30 & 1.58 & -9.3 & -14.2 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.084 & 30.8 & 6.2 & 4.3 & 6.0 & -1.5 & 21.0 & 1.49 & 8.30 & -6.9 & -9.9 \\  & \textit{Stable} & 1.100 & 37.2 & 7.1 & 4.4 & 7.2 & -1.1 & 34.5 & 0.84 & 1.95 & -9.5 & -11.3 \\  \hline \end{tabular} \label{tab:LLM_portfolio_statistics}
\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\scriptsize\textit{Note: 
Portfolio statistics of the trading strategy applied to the LLM clusters.
The statistics provided include performance metrics (Cumulative Return, Average Return), risk measures (Standard Deviation, Maximum Drawdown, Value at Risk, Conditional Value at Risk), risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics (Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable. 
Except for the Cumulative Return, all returns are annualized. The Sharpe Ratio is computed using the daily returns, assuming 252 trading days in a year. The Sortino Ratio is calculated using the daily downside returns. The Maximum Drawdown is the maximum loss from a peak to a trough. The Calmar Ratio is the ratio of the annualized return to the maximum drawdown. Skewness measures the asymmetry of the return distribution, while Kurtosis quantifies the tails' thickness. The Value at Risk (VaR) and Conditional Value at Risk (CVaR) are calculated at a 95\% confidence level.
The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-average-$SR$ in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs (shorts) clusters that minimize the rank difference between the training and validation rankings of the cluster-average-$SR$'s subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both algorithms, the cardinality of each leg is upper-bounded by a hyperparameter $\theta$. 
The holding period of the beta-neutral positions is set to $L$ = 4 trading days and the number of traded clusters is, $\theta = 0.5k=13$ as there are $k^*=26$ KMeans clusters of article embeddings. The selection criteria for these hyperparameters ($L,\theta$) is based on maximizing the Sharpe Ratios of the train and validation samples.
}
\end{minipage}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[H]
 \caption{Statistics of $\mathcal{P}_{\text{KMeans}}$ across data splits}
\centering
\renewcommand{\arraystretch}{1.1}  % Increased line spacing
% Define new column types for better spacing
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{tabular}{
    P{1.5cm}  % Split
    P{1.4cm}  % Algorithm
    P{1.2cm}  % Cum. Return
    P{1.1cm}  % Avg. Return
    P{1.1cm}  % St. Deviation
    P{1.3cm}  % Sharpe Ratio
    P{1.3cm}  % Sortino Ratio
    P{1.2cm}  % Max. Drawdown
    P{1.3cm}  % Calmar Ratio
    P{1.2cm}  % Skewness
    P{1.1cm}  % Kurtosis
    P{1.2cm}  % VaR
    P{1.2cm}  % CVaR
}
\Xhline{2\arrayrulewidth}
\textbf{Split} & \textbf{Algo.} & \textbf{Cum. Ret.} & \textbf{Avg. Ret.} & \textbf{St. Dev.} & \textbf{Sharpe Ratio} & \textbf{Sortino Ratio} & \textbf{Max. DD} & \textbf{Calmar Ratio} & \textbf{Skew.} & \textbf{Exc. Kurt.} & \textbf{VaR 95\%} & \textbf{CVaR 95\%} \\
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{All} & \textit{Greedy} & 1.070 & 5.3 & 9.7 & 0.5 & 0.6 & -6.9 & 0.8 & -0.45 & 4.04 & -13.7 & -22.9 \\  & \textit{Stable} & 1.489 & 35.8 & 16.8 & 1.8 & 2.2 & -7.6 & 4.7 & 0.19 & 5.08 & -22.6 & -36.1 \\  \hline \multirow{2}{*}{Train} & \textit{Greedy} & 0.969 & -4.6 & 11.6 & -0.4 & -0.4 & -6.5 & -0.7 & -0.54 & 2.86 & -18.2 & -28.5 \\  & \textit{Stable} & 1.285 & 46.3 & 19.3 & 2.0 & 2.4 & -7.6 & 6.1 & -0.20 & 3.49 & -28.9 & -41.6 \\  \hline \multirow{2}{*}{Validation} & \textit{Greedy} & 1.088 & 26.6 & 7.3 & 3.2 & 3.7 & -3.5 & 7.7 & -0.47 & 1.16 & -9.5 & -15.9 \\  & \textit{Stable} & 1.149 & 47.7 & 13.3 & 2.9 & 3.4 & -3.6 & 13.1 & -0.20 & 1.76 & -18.3 & -28.1 \\  \hline \multirow{2}{*}{Test} & \textit{Greedy} & 1.014 & 4.9 & 6.9 & 0.7 & 1.0 & -3.6 & 1.4 & 1.85 & 5.50 & -7.8 & -9.7 \\  & \textit{Stable} & 1.008 & 2.9 & 14.3 & 0.2 & 0.3 & -4.6 & 0.6 & 2.46 & 14.57 & -18.9 & -26.8 \\  \hline \end{tabular} \label{tab:KMeans_portfolio_statistics}
\vspace{0.5cm}
\begin{minipage}{\textwidth}
\setlength{\parindent}{0pt}
\scriptsize\textit{Note: 
Portfolio statistics of the trading strategy based on clusters obtained from applying KMeans to article embeddings. 
The statistics provided include performance metrics (Cumulative Return, Average Return), risk measures (Standard Deviation, Maximum Drawdown, Value at Risk, Conditional Value at Risk), risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics (Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable. 
Except for the Cumulative Return, all returns are annualized. The Sharpe Ratio is computed using the daily returns, assuming 252 trading days in a year. The Sortino Ratio is calculated using the daily downside returns. The Maximum Drawdown is the maximum loss from a peak to a trough. The Calmar Ratio is the ratio of the annualized return to the maximum drawdown. Skewness measures the asymmetry of the return distribution, while Kurtosis quantifies the tails' thickness. The Value at Risk (VaR) and Conditional Value at Risk (CVaR) are calculated at a 95\% confidence level.
The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-average-$SR$ in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs (shorts) clusters that minimize the rank difference between the training and validation rankings of the cluster-average-$SR$'s subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both algorithms, the cardinality of each leg is upper-bounded by a hyperparameter $\theta$. 
The holding period of the beta-neutral positions is set to $L$ = 4 trading days and the number of traded clusters is $\theta = 0.5k=13$ as there are $k^*=26$ KMeans clusters of article embeddings. The selection criteria for these hyperparameters ($L,\theta$) is based on maximizing the Sharpe Ratios of the train and validation samples.
}
\end{minipage}
\end{table}
\end{landscape}



\end{document}







