%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% INTRODUCING THE LITERATURE %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The foundation of textual analysis in finance, particularly in the predictive analysis of stock returns using economic news, is built upon several key studies that have shaped the current understanding and methodologies employed in this field. These seminal works not only introduced innovative approaches to analyzing textual data but also laid the groundwork for subsequent research that leverages advanced computational techniques.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mx
\cite{tetlock2007giving}
's study is a pivotal contribution to textual analysis in finance. He examines the predictive power of negative words in a popular column from The Wall Street Journal on subsequent stock returns. His findings indicate that high levels of media pessimism predict lower future returns, suggesting that media sentiment embedded in news content can significantly influence market prices. This research underscores the role of media tone and sentiment in shaping investor behavior and market trends.

\mx
\cite{fang2009media}
explore the impact of media coverage on the stock returns of publicly traded firms. They find that stocks with less media coverage generate higher future returns compared to those with more extensive coverage. This effect is attributed to the media's role in disseminating information to the public, which influences investor awareness and risk perceptions. Their study provides crucial insights into how variations in media attention can affect market dynamics and investor behavior.

\mx
\cite{bollen2011twitter} 
extend the domain of textual analysis into the realm of social media, demonstrating how sentiment derived from Twitter feeds can forecast stock market movements. They analyze the correlation between collective mood states expressed on Twitter and the Dow Jones Industrial Average. Their results suggest that certain mood dimensions on Twitter can precede and predict changes in stock prices, highlighting the increasing relevance of social media sentiment in financial forecasting.

\mx
\cite{jegadeesh2013word} 
introduce a refined content analysis technique that quantifies the informational value of words in financial documents, such as 10-K filings. Their method prioritizes term relevance based on the market's response to these filings, moving beyond traditional sentiment dictionaries to identify terms that significantly impact stock returns. The study demonstrates that the market reacts more predictively to the nuanced term-weighting approach, highlighting its effectiveness in extracting financially relevant information from corporate disclosures. This innovative methodology provides deeper insights into the influence of textual nuances on market behavior.

\mx
\cite{baker2016measuring}
introduce the Economic Policy Uncertainty Index, a novel quantitative measure constructed from the frequency of newspaper coverage concerning economic uncertainty and policy matters. The authors employ this index to explore the broader economic impacts of policy uncertainty, demonstrating its significant correlation with reduced macroeconomic performance, such as lower investment levels and reduced employment rates. While not directly focused on stock market predictability, this research is pivotal for its methodological innovation in quantifying an abstract concept like uncertainty using textual analysis. The EPU Index's ability to capture the macroeconomic climate influenced by policy decisions makes it a crucial tool for understanding market conditions that indirectly affect stock valuations and investor behavior, thus providing essential context for any analysis related to financial markets and economic forecasting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%   TRANSITION   %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As the field evolved, newer research began to address some of the limitations and assumptions inherent in earlier studies. More recently, advancements in machine learning and natural language processing have allowed researchers to dissect and leverage textual data with unprecedented granularity and accuracy. This shift towards more sophisticated models is exemplified by recent works that integrate deep learning technologies to parse and analyze economic news for market predictions more effectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mx 
%------------------  GPT PARSED ---------------------
% Ke, Z. T., Kelly, B. T., & Xiu, D. (2019). Predicting returns with text data (No. w26186). National Bureau of Economic Research.
%----------------------------------------------------

\cite{ke2019predicting} introduce a novel text-mining methodology that integrates machine learning techniques to predict asset returns from financial news articles. Their method, termed Sentiment Extraction via Screening and Topic Modeling (SESTM), innovates by constructing a sentiment scoring model tailored specifically for return prediction. SESTM operates through three steps: predictive screening to identify relevant terms, assigning weights to these terms using a supervised topic model, and aggregating these into an article-level predictive score through penalized likelihood. The empirical analysis, which leverages the Dow Jones Newswires, demonstrates that this model effectively extracts predictive signals from news content, highlighting that news assimilation into prices occurs with a delay, particularly for smaller and more volatile firms. While the SESTM methodology showcases significant advancements in return predictability using textual data, it primarily focuses on extracting general sentiment rather than dissecting the specific financial impacts of news content on stock prices. This approach may overlook the nuanced ways in which individual news items affect specific firms, an area where the methodology in this paper aims to contribute. 

%By employing a function schema that instructs GPT to analyze and parse news for detailed, firm-specific financial impacts, this paper proposes a more targeted analysis, enhancing the granularity and applicability of news-driven financial predictions. This method not only deepens the level of textual analysis beyond sentiment scoring but also aligns closely with practical needs for precision in financial decision-making.


\mx 
%------------------  GPT PARSED ---------------------
% Baker, S., Bloom, N., Davis, S. J., & Sammon, M. C. (2021). What triggers stock market jumps?. [National Bureau of Economic Research]
%----------------------------------------------------

%\cite{baker2021triggers}
%They integrate news text into macro-finance analyses using carefully curated researcher inputs in place of statistical models. (i.e: train graduate students to read news article and ask them to extract the sentiment from many news articles. This is an expensive and brute force approach. May be contaminated by human error and biases)
%Interesting: They analyze around 6000 news, but using human readers! In our case, we can do this for a big amount of news using GPT

\cite{baker2021triggers} utilizes a detailed analysis of newspaper accounts to determine the proximate causes of significant stock market jumps. Their method employs human readers to categorize 6,200 instances of market jumps from 16 national markets into 17 predefined categories, based on journalistic accounts. While this approach has the advantage of detailed human interpretation, it is susceptible to several issues: the cost and time required to train and deploy human coders are significant, and human bias may affect the consistency and neutrality of data categorization, as readers may inadvertently favor narratives that fit with prevailing economic theories or personal biases. Additionally, human coding may lack scalability and flexibility when adapting to new information or changing market dynamics. Using a Large Language Model for parsing news offers a compelling alternative. LLMs can process large volumes of data at scale and with consistent criteria, reducing the subjective bias introduced by human readers thus making it a more robust and cost-effective tool for financial analysis compared to the labor-intensive process of human coding.


%\mx 
%%------------------  GPT PARSED ---------------------
%% Kelly, B., Manela, A., & Moreira, A. (2021). Text selection. Journal of Business & Economic Statistics
%%----------------------------------------------------
%\cite{kelly2021text} introduce a novel approach to economic textual analysis with their Hurdle Distributed Multinomial Regression (HDMR) model. This model innovatively addresses the issue of sparsity in high-dimensional text data by focusing on the inclusion rather than frequency of phrases, aiming to capture economically significant phrases from a broad corpus. They demonstrate the model's efficacy through applications like analyzing U.S. Congressional speeches for partisanship and forecasting economic indicators from newspaper text, showing a marked improvement in predictive performance and interpretability over traditional models. However, while HDMR effectively handles text sparsity and enhances model interpretability, it may fall short in adapting to new data or evolving linguistic trends due to its static nature. Each phrase's significance is determined at the time of model training, potentially overlooking emerging terms or phrases that could gain economic significance later on. In contrast, the methodology in this paper, utilizing the dynamic capabilities of the GPT model, continuously adapts to new language usage and evolving contexts, maintaining its relevance and accuracy over time in financial news analysis. This adaptability makes it particularly suited to the fast-paced environment of stock market forecasting, where the relevance and impact of news can shift rapidly.



\mx 
%------------------  GPT PARSED ---------------------
%Jiang, H., Li, S. Z., & Wang, H. (2021). Pervasive underreaction: Evidence from high-frequency data. Journal of Financial Economics, 141(2), 573-599.
%----------------------------------------------------
%\cite{jiang2021pervasive} decompose daily stock returns into news-and
%non-news-driven components, and uncover evidence of pervasive stock market underreaction to firm news.

\cite{jiang2021pervasive} delve into the dynamics of stock market reactions to firm news, proposing a novel approach to decompose daily stock returns into news-driven and non-news-driven components. Utilizing high-frequency data, the authors analyze the intraday price movements following firm-specific news, identifying a significant underreaction to news events. They document that stock prices tend to continue moving in the direction of the initial reaction for several days without reversing, and develop a trading strategy that exploits this return drift, yielding high abnormal returns even after accounting for transaction costs. While the study presents robust findings on market underreaction and its profitable exploitation using high-frequency data, it primarily focuses on the aggregate behavior of the market rather than the specific impacts on individual firms. In this paper, by looking at the specific impacts on individual firms we obtain tailored insights for individual stock predictions, potentially providing a deeper understanding of the financial implications of news events on specific firms. 

\mx 
%------------------  GPT PARSED ---------------------
% Bybee, L., Kelly, B. T., Manela, A., & Xiu, D. (2021). Business news and business cycles (No. w29344). National Bureau of Economic Research.	
%----------------------------------------------------

%\cite{bybee2021business} also perform textual analyisis of business news, but  in their case, they employ a topic model based on a bag-of-words approach, which is an unsupervised tecnhique for dimension reduction and clustering. Hence, their analysis of news doesn't take into accound the contextual nature and semantic relationships of text. 
%*-*-*-*-*-*-*-*-*-*-
%\cite{bybee2021business} employ Latent Dirichlet Allocation (LDA) to distill the content of approximately 800,000 articles from The Wall Street Journal spanning from 1984 to 2017 into a manageable number of topics that represent distinct thematic elements of business news. These topics are then used to measure the share of media attention across various economic issues over time. The study demonstrates that shifts in the thematic focus of news coverage closely correlate with macroeconomic activity and can significantly explain variations in aggregate stock market returns. This methodology showcases the potential of text-based data to capture the latent states of economic conditions that are not directly observable through traditional economic indicators.
% However, a notable limitation of LDA is its bag-of-words approach, which overlooks the context in which terms appear. This can lead to a loss of nuanced meaning since the semantic relationships and dependencies between words are not considered, potentially reducing the accuracy of interpreting the impact of news.
%*-*-*-*-*-*-*-*-*-*-
\cite{bybee2021business} employ Latent Dirichlet Allocation (LDA) to process approximately 800,000 articles from The Wall Street Journal spanning from 1984 to 2017. By distilling large volumes of text into interpretable topical themes, they quantify the proportion of news attention allocated to each theme over time, effectively demonstrating that variations in these thematic exposures closely track economic activities and can explain about 25\% of aggregate stock market returns. This approach effectively harnesses the vast information contained in news text to model macroeconomic dynamics and demonstrates the significant role of media in shaping economic perceptions. However, the methodology relies on a bag-of-words model, which overlooks the syntactic and contextual nuances of language, potentially compromising the depth and accuracy of the analysis. In contrast, the novel methodology proposed in this paper, which exploits the capabilities of LLMs to parse news articles, maintains the contextual integrity of data, allowing for a more nuanced and precise understanding of how specific news articles influence individual stock prices.



\mx 
%------------------  GPT PARSED ---------------------
% Expected Returns and Large Language Models (B. Kelly, D. Xiu) SSRN
%----------------------------------------------------

%\cite{chen2022expected} do a thorough job in analyzing the market timing abilities of LLMs, however, in their quest for explainability, they employ old-fashioned technology that is completely deprecated. In their paper, they renegate of GPT as a tool for explicit generation of trading sginals, since "it has not been trained for this purpose". However, they do recognize the promising ability of GPT as a news parser, which could then be used to launch optimal trading signals.
\cite{chen2022expected} present a comprehensive analysis of market timing capabilities using LLMs, emphasizing their efficacy in extracting and modeling stock returns from financial news. The authors maintain a reliance on more traditional technologies such as BERT, RoBERTa, and OPT, for explicating model behaviors, potentially limiting the adaptability and future readiness of their approach.
%However, the authors acknowledge the potential of employing more sophisticated LLMs, though they 
%Furthermore, they explicitly acknowledge limitations in using GPT for direct trading signal generation, citing its original training not being aligned with specific financial tasks, yet they recognize GPT's potential as a sophisticated news parser.
In contrast, this paper leverages the parsing capabilities of cutting-edge LLMs (we employ the LLaMA-3 model, released in April 2024) and we employ a function-calling approach to tailor and structure the economic analysis of business news directly for market prediction. Our innovative use of function calls allows us to transform the raw analytical power of LLMs into a targeted tool for generating actionable trading insights. 
%By doing so, we not only harnesses the advanced contextual comprehension but also directs its output to effectively inform trading decisions, showcasing a practical and forward-thinking application of LLMs in financial markets. This method stands to offer more direct and dynamic market insights compared to the somewhat static and general-purpose models discussed in \cite{chen2022expected}, presenting a novel pathway to integrating AI in financial analysis and decision-making.

\mx 
%------------------  GPT PARSED ---------------------
% Bybee, L.. (2023).  The Ghost in the Machine: Generating Beliefs with Large Language Models
%----------------------------------------------------

\cite{bybee2023ghost} use GPT-3.5 to generate economic expectations from historical news data from The Wall Street Journal. By feeding GPT-3.5 with past news articles, the author creates a structured time series of economic beliefs that closely match traditional economic surveys. His study demonstrates the potential of LLMs to replicate and extend traditional survey measures by analyzing aggregate economic behavior over extensive historical periods. 
While \cite{bybee2023ghost} offers a robust framework for examining aggregate economic behaviors through a macro lens, the methodology in this paper adopts a different approach by focusing on the firm-specific impact of business news on stock prices. This involves a detailed parsing of news content to extract actionable financial insights directly related to market behavior by targeting individual stock reactions rather than broad economic sentiment.
%, this paper offers a more granular and direct analysis, better suited to investors and financial analysts seeking precise cues from daily news flows.


\mx 
%------------------  GPT PARSED ---------------------
% Lopez-Lira, A., & Tang, Y. (2023). Can chatgpt forecast stock price movements? return predictability and large language models. arXiv preprint arXiv:2304.07619.
%----------------------------------------------------

%More recently, \cite{lopez2023can} employed ChatGPT to make stock market predictions based on news headlines. However, these trading signals are overly limited by the nature of their dataset, news headlines, compared to using full news articles as we do in this paper. Also, their data doesn't allow to test for lookahead bias in GPT, since they only consider data after the last training period of GPT (September 2021). Furthermore, aware of the restriction imposed by working only with headlines, the authors limit themselves to asking ChatGPT to complete a sentence after having fed it with an article; such masked phrase is ``This is \_ news''. They extract the trading signal from such completion: "good", "no", "bad". Their methodology is unsophisticated and does not 
%make justice to the potential of state-of-the-art LLMs in timing the market.
%By designing a function schema that guides GPT in parsing the news, we obtain detailed information on the nature of the news article in order to obtain a much higher-quality trading signal from GPT.


More recently, \cite{lopez2023can} investigated the use of ChatGPT for stock market predictions by analyzing news headlines. They employed a dataset consisting solely of headlines from major news sources, analyzed post-September 2021, to avoid overlap with ChatGPT's training data. 
%This temporal restriction, however, prevents them from testis lookahead bias, an interesting exercise in this type of analysis.
%Furthermore, recognizing the constraints of working solely with headlines, the authors confine their approach to prompting ChatGPT to complete a sentence after having fed it with a headline. The masked phrase used is "This is \_ news", and they derive the trading signal from such completion: "good," "no," "bad." This methodology, while straightforward, fails to fully capitalize on the sophisticated capabilities of state-of-the-art large language models (LLMs) in market timing.
The authors' methodology involved prompting ChatGPT to classify these headlines as \qquote{good}, \qquote{neutral}, or \qquote{bad} for stock prices, based on a simplistic sentence-completion task where the model fills in a masked phrase, \qquote{This is \_ news}. This approach does not fully harness the capabilities of state-of-the-art LLMs for deep textual analysis and market timing. It also confines the analysis to the superficial information available in headlines rather than detailed reports. In contrast, this paper employs a more sophisticated method by designing a function schema that guides the LLM in parsing and analyzing full news articles. This methodology not only allows for a deeper and more nuanced understanding of news content but also enhances the quality of the trading signals derived from the LLM's interpretation. By using full articles instead of headlines, we obtain more reliable and actionable insights for market timing decisions.


\mx 
%----------------------------------------------------
% ChatGPT, Stock Market Predictability and Links to the Macroeconomy. SSRN
%----------------------------------------------------
%\cite{chen2023chatgpt}
%Only employ news headlines and alerts from the Wall Street Journal and focus on the examination of the timing ability of ChatGPT on the aggregate stock market (and not on the specific firmst affected by the news). Their analysis is based on a single prompt where ChatGPT is asked to read the news and say whether the stock market would GO UP, GO DOWN or UNKNOWN. Again, they don't exploit GPT's intrinsic abilities to produce a rigorous evaluation of news content, and they simply judge this technology on its capacity to ''time the aggregate stock market'', an ability for which it has not been trained and is not expected to excel at. In contrast, in this paper, we direct GPT's through a function schema that rigorously instructs it to analyze and parse the news. Our approach permits a targeted analysis that informs better trading decisions.

\cite{chen2023chatgpt} utilize news headlines and alerts from the Wall Street Journal to assess the aggregate market timing capabilities of ChatGPT, disregarding the effects on specific firms mentioned in the news.  
Their methodology hinges on a singular prompt: ChatGPT is tasked with reading a headline and predicting whether the stock market will \qquote{go up}, \qquote{go down}, or remain \qquote{unknown}.
This approach fails to leverage GPT's sophisticated abilities for a deep evaluation of news content. Instead, it narrowly evaluates the model's ability to time the market, an area outside its training and not aligned with its core strengths. Conversely, this paper exploits the use of advanced LLMs by employing a function schema that meticulously guides the model to comprehensively analyze and parse news articles. This structured methodology fosters a more precise and targeted analysis, yielding superior trading insights and decisions based on firm-specific impacts rather than broad market movements.

 
