%\input{alg_SC-SCM.tex}
% Conversation with Claude: https://claude.ai/chat/dcf3ca08-a154-414b-bcab-0afb3cb09877

\section{Theoretical Framework}
This section will establish the mathematical foundation of our approach, including:
\begin{itemize}
    \item The probability space and filtration
    \item The return processes for both target and donor assets
    \item The formal definition of both linear and nonlinear synthetic control estimators
    \item The theoretical properties of the deviation process
\end{itemize}

\section{Model Construction and Estimation}
Here we will detail:
\begin{itemize}
    \item The linear SCM optimization problem with regularization
    \item The nonlinear SCM framework using neural networks
    \item The estimation procedure for both approaches
    \item Cross-validation and hyperparameter selection methods
    \item Model validation metrics and diagnostics
\end{itemize}

\section{Signal Generation Framework}
This section will cover:
\begin{itemize}
    \item The construction of the deviation process
    \item The rolling statistics computation methodology
    \item The standardization procedure
    \item The theoretical properties of our Z-score process
    \item Statistical tests for mean reversion in the Z-score
\end{itemize}

\section{Trading Strategy Implementation}
We will detail:
\begin{itemize}
    \item The threshold-based trading rules
    \item Position sizing methodology based on signal strength
    \item The portfolio construction process for both linear and nonlinear cases
    \item Transaction cost considerations
    \item Implementation details for the two proposed approaches to trading nonlinear synthetic controls
\end{itemize}

\section{Risk Management Framework}
This comprehensive section will address:
\begin{itemize}
    \item Position-level risk controls including stop-loss limits
    \item Portfolio-level risk constraints
    \item Market neutrality considerations
    \item Factor exposure management
    \item Correlation-based risk limits
    \item Value-at-Risk frameworks
\end{itemize}

\section{Performance Analytics}
The final section will cover:
\begin{itemize}
    \item Return attribution methodology
    \item Risk-adjusted performance metrics
    \item Transaction cost analysis
    \item Statistical significance tests
    \item Robustness checks
\end{itemize}


\newpage
\section{Theoretical Framework}

Let $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t\geq 0}, \mathbb{P})$ be a filtered probability space, where $\{\mathcal{F}_t\}_{t\geq 0}$ represents the natural filtration generated by our market processes. We consider a financial market with a universe of assets $\mathcal{I} := \{0,1,\ldots,N\}$, where asset 0 represents our target instrument and assets $\{1,\ldots,N\}$ constitute our donor pool.

For each asset $i \in \mathcal{I}$, let $R_{i,t}$ denote its return at time $t$. We assume these returns are adapted to the filtration $\{\mathcal{F}_t\}_{t\geq 0}$. For notational convenience, we define $\mathbf{R}_t = (R_{1,t},\ldots,R_{N,t})'$ as the vector of donor asset returns at time $t$.

\subsection{Synthetic Control Estimators}

We consider two classes of synthetic control estimators: linear and nonlinear. Both approaches aim to construct a synthetic version of the target asset's returns, but differ in their structural assumptions and implementation.

\subsubsection{Linear Synthetic Control}

The linear synthetic control estimator takes the form:
\begin{equation}
    \hat{R}_{0,t} = \mathbf{w}'\mathbf{R}_t
\end{equation}
where $\mathbf{w} \in \mathbb{R}^N$ is a vector of weights chosen to minimize the regularized tracking error:
\begin{equation}
    \min_{\mathbf{w}} \|\mathbf{R}_0 - \mathbf{R}\mathbf{w}\| + \lambda\mathcal{R}(\mathbf{w}) 
    \quad \text{subject to} \quad \mathbf{1}'\mathbf{w} = 1
\end{equation}
Here, $\mathcal{R}(\mathbf{w})$ represents a regularization term that can take various forms, such as:
\begin{itemize}
    \item LASSO (L1): $\mathcal{R}(\mathbf{w}) = \|\mathbf{w}\|_1$
    \item Ridge (L2): $\mathcal{R}(\mathbf{w}) = \|\mathbf{w}\|_2^2$
    \item Elastic Net: $\mathcal{R}(\mathbf{w}) = \alpha\|\mathbf{w}\|_1 + (1-\alpha)\|\mathbf{w}\|_2^2$
\end{itemize}

\subsubsection{Nonlinear Synthetic Control}

The nonlinear synthetic control estimator extends the framework by allowing for more complex relationships:
\begin{equation}
    \hat{R}_{0,t} = f_{\boldsymbol{\theta}}(\mathbf{R}_t)
\end{equation}
where $f_{\boldsymbol{\theta}}: \mathbb{R}^N \rightarrow \mathbb{R}$ is a nonlinear function parameterized by $\boldsymbol{\theta}$, typically implemented as a neural network. The parameters $\boldsymbol{\theta}$ are chosen to minimize:
\begin{equation}
    \min_{\boldsymbol{\theta}} \frac{1}{T_{tr}}\sum_{t\in\mathcal{T}_{tr}} L(R_{0,t}, f_{\boldsymbol{\theta}}(\mathbf{R}_t)) + \lambda\mathcal{R}(\boldsymbol{\theta})
\end{equation}
where $L(\cdot,\cdot)$ is a suitable loss function and $\mathcal{R}(\boldsymbol{\theta})$ is a regularization term on the network parameters.

\subsection{Deviation Process}

The core of our statistical arbitrage strategy relies on analyzing the deviations between the target asset's returns and its synthetic counterpart. We define the deviation process as:
\begin{equation}
    \delta_t = R_{0,t} - \hat{R}_{0,t}
\end{equation}

Under the assumption that the synthetic control effectively captures the systematic components of the target asset's returns, we can decompose $\delta_t$ as:
\begin{equation}
    \delta_t = \alpha_t + \epsilon_t
\end{equation}
where $\alpha_t$ represents a potentially time-varying drift term and $\epsilon_t$ is a mean-reverting noise process.

The statistical arbitrage opportunity arises from the mean-reverting nature of $\delta_t$. We formalize this by assuming that $\delta_t$ follows a general mean-reverting process:
\begin{equation}
    d\delta_t = \kappa(\mu - \delta_t)dt + \sigma_t dW_t
\end{equation}
where $\kappa > 0$ is the mean-reversion speed, $\mu$ is the long-term mean level, $\sigma_t$ is the instantaneous volatility, and $W_t$ is a standard Brownian motion.

This theoretical framework provides the foundation for our trading strategy, which we will develop in subsequent sections. The mean-reverting nature of $\delta_t$ suggests that significant deviations from the synthetic control value represent temporary mispricings that can be exploited through appropriate trading rules.


\section{Model Construction and Estimation}

The implementation of our statistical arbitrage strategy requires careful consideration of how we construct and estimate both the linear and nonlinear synthetic control models. This section details the estimation procedures, validation methods, and practical considerations for both approaches.

\subsection{Linear Synthetic Control Model}

\subsubsection{Model Specification}
For a training window of size $M$, let $\mathbf{R}_0 \in \mathbb{R}^M$ denote the vector of target asset returns and $\mathbf{R} \in \mathbb{R}^{M \times N}$ the matrix of donor asset returns. The linear synthetic control weights are obtained by solving:

\begin{equation}
\begin{aligned}
    \min_{\mathbf{w}} & \quad \|\mathbf{R}_0 - \mathbf{R}\mathbf{w}\|_2^2 + \lambda\mathcal{R}(\mathbf{w}) \\
    \text{s.t.} & \quad \mathbf{1}'\mathbf{w} = 1
\end{aligned}
\end{equation}

We implement this optimization with elastic net regularization:
\begin{equation}
    \mathcal{R}(\mathbf{w}) = \alpha\|\mathbf{w}\|_1 + (1-\alpha)\|\mathbf{w}\|_2^2
\end{equation}
where $\alpha \in [0,1]$ controls the balance between L1 and L2 regularization.

\subsubsection{Rolling Window Estimation}
For each time $t$, we estimate the model using data from the window $[t-M+1, t]$:

\begin{equation}
    \mathbf{w}_t = \argmin_{\mathbf{w}} \left\{\sum_{s=t-M+1}^t (R_{0,s} - \mathbf{w}'\mathbf{R}_s)^2 + \lambda\mathcal{R}(\mathbf{w}) \quad \text{s.t.} \quad \mathbf{1}'\mathbf{w} = 1\right\}
\end{equation}

\subsection{Nonlinear Synthetic Control Model}

\subsubsection{Neural Network Architecture}
We implement the nonlinear function $f_{\boldsymbol{\theta}}$ as a feedforward neural network with $L$ layers. The network architecture is defined recursively as:

\begin{equation}
\begin{aligned}
    \mathbf{h}^{(0)} &= \mathbf{R}_t \\
    \mathbf{h}^{(l)} &= \sigma^{(l)}(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}), \quad l = 1,\ldots,L-1 \\
    f_{\boldsymbol{\theta}}(\mathbf{R}_t) &= \mathbf{W}^{(L)}\mathbf{h}^{(L-1)} + b^{(L)}
\end{aligned}
\end{equation}

where $\sigma^{(l)}$ represents the activation function (ReLU) for layer $l$, and $\boldsymbol{\theta} = \{\mathbf{W}^{(l)}, \mathbf{b}^{(l)}\}_{l=1}^L$ comprises all network parameters.

\subsubsection{Training Procedure}
For each time $t$, we train the network by minimizing:
\begin{equation}
    \mathcal{L}_t(\boldsymbol{\theta}) = \frac{1}{M}\sum_{s=t-M+1}^t (R_{0,s} - f_{\boldsymbol{\theta}}(\mathbf{R}_s))^2 + \lambda\|\boldsymbol{\theta}\|_2^2
\end{equation}

The optimization is performed using mini-batch stochastic gradient descent with the Adam optimizer:
\begin{equation}
    \boldsymbol{\theta}_t^{(k+1)} = \boldsymbol{\theta}_t^{(k)} - \eta_k\nabla_{\boldsymbol{\theta}}\mathcal{L}_t(\boldsymbol{\theta}_t^{(k)})
\end{equation}
where $\eta_k$ is a learning rate schedule.

\subsection{Model Validation and Selection}

\subsubsection{Cross-Validation Framework}
We employ a time-series cross-validation approach to select hyperparameters:

\begin{enumerate}
    \item Split the training window into $K$ contiguous blocks
    \item For each candidate hyperparameter set $\lambda \in \Lambda$:
        \begin{itemize}
            \item For $k = 1,\ldots,K$:
                \begin{itemize}
                    \item Train model on blocks $1,\ldots,k-1$
                    \item Validate on block $k$
                    \item Compute validation error $e_k(\lambda)$
                \end{itemize}
            \item Compute average validation error: $\bar{e}(\lambda) = \frac{1}{K}\sum_{k=1}^K e_k(\lambda)$
        \end{itemize}
    \item Select $\lambda^* = \argmin_{\lambda \in \Lambda} \bar{e}(\lambda)$
\end{enumerate}

\subsubsection{Performance Metrics}
We evaluate model performance using multiple metrics:

\begin{equation}
\begin{aligned}
    \text{MSE} &= \frac{1}{T}\sum_{t=1}^T (R_{0,t} - \hat{R}_{0,t})^2 \\
    \text{MAE} &= \frac{1}{T}\sum_{t=1}^T |R_{0,t} - \hat{R}_{0,t}| \\
    R^2 &= 1 - \frac{\sum_{t=1}^T (R_{0,t} - \hat{R}_{0,t})^2}{\sum_{t=1}^T (R_{0,t} - \bar{R}_0)^2}
\end{aligned}
\end{equation}

\subsubsection{Model Diagnostics}
We perform several diagnostic checks:
\begin{equation}
\begin{aligned}
    \text{Autocorrelation}: &\quad \rho_k = \text{Corr}(\delta_t, \delta_{t-k}) \\
    \text{Heteroskedasticity}: &\quad \text{ARCH-LM test on } \delta_t \\
    \text{Normality}: &\quad \text{Jarque-Bera test on } \delta_t
\end{aligned}
\end{equation}

These diagnostics help ensure the reliability of our synthetic control estimates and inform the construction of trading signals in subsequent stages.

\section{Signal Generation Framework}

The signal generation framework transforms the deviations between actual and synthetic returns into standardized trading signals. This section details the construction and statistical properties of these signals.

\subsection{Construction of Trading Signals}

\subsubsection{Deviation Process}
We begin by computing the deviation between the target asset's return and its synthetic counterpart:
\begin{equation}
    \delta_t = R_{0,t} - \hat{R}_{0,t}
\end{equation}

The sign of $\delta_t$ carries important information:
\begin{itemize}
    \item $\delta_t > 0$ indicates that the target asset's return exceeds its synthetic estimate
    \item $\delta_t < 0$ indicates that the target asset's return falls below its synthetic estimate
\end{itemize}

\subsubsection{Rolling Statistics}
For a rolling window of size $N$, we compute the following statistics at each time $t$:

\begin{equation}
\begin{aligned}
    \mu_t(\delta) &= \frac{1}{N}\sum_{s=t-N+1}^t \delta_s \\
    \sigma_t(\delta) &= \sqrt{\frac{1}{N-1}\sum_{s=t-N+1}^t (\delta_s - \mu_t(\delta))^2}
\end{aligned}
\end{equation}

The rolling window size $N$ is chosen to balance between:
\begin{itemize}
    \item Signal stability (larger $N$)
    \item Responsiveness to regime changes (smaller $N$)
\end{itemize}

\subsubsection{Standardized Score}
We construct the standardized score:
\begin{equation}
    Z_t = \frac{\delta_t - \mu_t(\delta)}{\sigma_t(\delta)}
\end{equation}

Under suitable conditions, $Z_t$ approximately follows a standard normal distribution, providing a natural scale for calibrating trading thresholds.

\subsection{Statistical Properties}

\subsubsection{Mean Reversion Tests}
To validate the mean-reverting nature of the deviation process, we conduct several statistical tests:

\begin{enumerate}
    \item Augmented Dickey-Fuller test for stationarity:
    \begin{equation}
        \Delta \delta_t = (\rho-1)\delta_{t-1} + \sum_{j=1}^p \gamma_j \Delta \delta_{t-j} + \epsilon_t
    \end{equation}
    
    \item Variance ratio test for mean reversion:
    \begin{equation}
        VR(q) = \frac{\text{Var}(\delta_t(q))}{q\text{Var}(\delta_t(1))}
    \end{equation}
    where $\delta_t(q)$ represents $q$-period changes in the deviation process.
    
    \item Hurst exponent estimation:
    \begin{equation}
        H = \log(R/S)_n/\log(n)
    \end{equation}
    where $(R/S)_n$ is the rescaled range statistic over $n$ periods.
\end{enumerate}

\subsubsection{Signal Properties}
The standardized score $Z_t$ exhibits several important properties:

\begin{equation}
\begin{aligned}
    \mathbb{E}[Z_t] &= 0 \\
    \text{Var}[Z_t] &= 1 \\
    \text{Skew}[Z_t] &= \frac{\mathbb{E}[(Z_t - \mathbb{E}[Z_t])^3]}{\text{Var}[Z_t]^{3/2}} \\
    \text{Kurt}[Z_t] &= \frac{\mathbb{E}[(Z_t - \mathbb{E}[Z_t])^4]}{\text{Var}[Z_t]^2}
\end{aligned}
\end{equation}

\subsection{Signal Calibration}

\subsubsection{Threshold Selection}
We define four critical thresholds for signal generation:
\begin{equation}
\begin{aligned}
    c_{\text{open-long}} &< 0 < c_{\text{close-long}} \\
    c_{\text{close-short}} &< 0 < c_{\text{open-short}}
\end{aligned}
\end{equation}

These thresholds are calibrated by:
\begin{enumerate}
    \item Setting initial values based on standard normal quantiles
    \item Optimizing based on historical performance metrics
    \item Adjusting for transaction costs and market impact
\end{enumerate}

\subsubsection{Dynamic Threshold Adjustment}
To account for changing market conditions, we implement dynamic threshold adjustment:
\begin{equation}
    c_t = c_{\text{base}} \cdot f(\sigma_t^{\text{market}})
\end{equation}
where $f(\cdot)$ is a scaling function and $\sigma_t^{\text{market}}$ is a measure of market volatility.

\subsection{Signal Quality Metrics}

We evaluate signal quality using several metrics:

\begin{equation}
\begin{aligned}
    \text{Signal-to-Noise Ratio} &= \frac{|\mathbb{E}[R_t | Z_t > c_{\text{open}}]|}{\sigma(R_t | Z_t > c_{\text{open}})} \\
    \text{Hit Rate} &= \mathbb{P}(\text{sign}(R_{t+1}) = \text{sign}(Z_t) | |Z_t| > c_{\text{open}}) \\
    \text{Information Coefficient} &= \text{Corr}(Z_t, R_{t+1})
\end{aligned}
\end{equation}

These metrics help assess the predictive power of our signals and inform potential refinements to the signal generation process.


\section{Trading Strategy Implementation}

This section details the implementation of our statistical arbitrage strategy, including position sizing, portfolio construction, and practical considerations for both linear and nonlinear synthetic control approaches.

\subsection{Trading Rules}

Based on the standardized score $Z_t$ and our calibrated thresholds, we define the trading position indicator $\phi_t \in \{-1, 0, 1\}$ as follows:

\begin{equation}
    \phi_t = \begin{cases}
        1 & \text{if } Z_t \leq c_{\text{open-long}} \text{ and } \phi_{t-1} = 0 \\
        0 & \text{if } Z_t \geq c_{\text{close-long}} \text{ and } \phi_{t-1} = 1 \\
        -1 & \text{if } Z_t \geq c_{\text{open-short}} \text{ and } \phi_{t-1} = 0 \\
        0 & \text{if } Z_t \leq c_{\text{close-short}} \text{ and } \phi_{t-1} = -1 \\
        \phi_{t-1} & \text{otherwise}
    \end{cases}
\end{equation}

\subsection{Position Sizing}

We implement a dynamic position sizing framework that accounts for both signal strength and risk considerations.

\subsubsection{Signal-Based Sizing}
The base position size is determined by the magnitude of the signal:

\begin{equation}
    \eta_t^{\text{signal}} = \eta_{\text{max}} \cdot \frac{1}{1 + e^{-\lambda(|Z_t| - c_{\text{open}})}}
\end{equation}

where:
\begin{itemize}
    \item $\eta_{\text{max}}$ is the maximum allowed position size
    \item $\lambda$ controls the sensitivity to signal strength
    \item The sigmoid function ensures smooth scaling and bounded positions
\end{itemize}

\subsubsection{Risk-Adjusted Sizing}
The final position size incorporates volatility scaling:

\begin{equation}
    \eta_t = \eta_t^{\text{signal}} \cdot \min\left\{1, \frac{\sigma_{\text{target}}}{\sigma_t(\delta)}\right\}
\end{equation}

where $\sigma_{\text{target}}$ is our target volatility level.

\subsection{Portfolio Construction}

\subsubsection{Linear Synthetic Control Implementation}
For the linear SCM case, we construct the portfolio as follows:

\begin{equation}
\begin{aligned}
    P_{0,t} &= \eta_t\phi_t && \text{(Position in target asset)} \\
    \mathbf{P}_{t} &= -\eta_t\phi_t\mathbf{w}_t && \text{(Position in donor assets)}
\end{aligned}
\end{equation}

The total portfolio return at time $t$ is:

\begin{equation}
    R_t^p = \eta_t\phi_t(R_{0,t} - \mathbf{w}_t'\mathbf{R}_t)
\end{equation}

\subsubsection{Nonlinear Implementation Approaches}

For the nonlinear SCM case, we present two implementation approaches:

\paragraph{Local Linear Approximation}
We approximate the nonlinear function locally using its gradient:

\begin{equation}
    \nabla f_{\boldsymbol{\theta}}(\mathbf{R}_t) = \left.\frac{\partial f_{\boldsymbol{\theta}}(\mathbf{R})}{\partial \mathbf{R}}\right|_{\mathbf{R}=\mathbf{R}_t}
\end{equation}

The hedging portfolio weights are then:

\begin{equation}
    \mathbf{w}_t^{\text{hedge}} = -\eta_t\phi_t\frac{\nabla f_{\boldsymbol{\theta}}(\mathbf{R}_t)}{\|\nabla f_{\boldsymbol{\theta}}(\mathbf{R}_t)\|_1}
\end{equation}

\paragraph{Beta-Adjusted Linear Hedge}
We estimate a rolling beta between the nonlinear synthetic returns and a linear combination:

\begin{equation}
    f_{\boldsymbol{\theta}}(\mathbf{R}_t) = \alpha_t + \beta_t(\mathbf{w}_t^{\text{linear}}{}'\mathbf{R}_t) + \epsilon_t
\end{equation}

The hedging portfolio is then constructed as:

\begin{equation}
    \mathbf{w}_t^{\text{hedge}} = -\eta_t\phi_t\beta_t\mathbf{w}_t^{\text{linear}}
\end{equation}

\subsection{Transaction Cost Management}

\subsubsection{Cost Model}
We model transaction costs as:

\begin{equation}
    TC_t = c\left(|P_{0,t} - P_{0,t-1}| + \|\mathbf{P}_t - \mathbf{P}_{t-1}\|_1\right) + \gamma(|P_{0,t} - P_{0,t-1}|^2 + \|\mathbf{P}_t - \mathbf{P}_{t-1}\|_2^2)
\end{equation}

where:
\begin{itemize}
    \item $c$ represents proportional costs (bid-ask spread, commissions)
    \item $\gamma$ captures market impact costs
\end{itemize}

\subsubsection{Trade Implementation}
To manage trading costs, we implement:

\begin{equation}
    \Delta P_t = \text{sign}(P_t^{\text{target}} - P_{t-1}) \cdot \min\{|P_t^{\text{target}} - P_{t-1}|, \Delta P_{\text{max}}\}
\end{equation}

where $\Delta P_{\text{max}}$ is the maximum allowed position change per period.

\subsection{Strategy Performance}

The net strategy return, accounting for transaction costs, is:

\begin{equation}
    R_t^s = R_t^p - TC_t
\end{equation}

Key performance metrics include:

\begin{equation}
\begin{aligned}
    \text{Sharpe Ratio} &= \frac{\mathbb{E}[R_t^s]}{\sqrt{\text{Var}(R_t^s)}} \\
    \text{Information Ratio} &= \frac{\mathbb{E}[R_t^s]}{\sqrt{\text{Var}(R_t^s - R_t^b)}} \\
    \text{Maximum Drawdown} &= \max_{t,s\leq t}\frac{V_s - V_t}{V_s}
\end{aligned}
\end{equation}

where $R_t^b$ represents an appropriate benchmark return.



\section{Risk Management Framework}

The risk management framework for our statistical arbitrage strategy encompasses multiple layers of controls designed to ensure portfolio stability and limit potential losses. This section details our systematic approach to identifying, measuring, and controlling various sources of risk.

\subsection{Position-Level Risk Controls}

\subsubsection{Stop-Loss Mechanisms}
We implement both absolute and relative stop-loss thresholds. For any open position, we define the cumulative profit and loss:

\begin{equation}
    \text{PnL}_t = \sum_{s=t_{\text{entry}}}^t R_s^p
\end{equation}

The position is automatically closed if either condition is met:
\begin{equation}
\begin{aligned}
    \text{Absolute Stop:} \quad & \text{PnL}_t < -L_{\text{max}} \\
    \text{Relative Stop:} \quad & \frac{\text{PnL}_t}{\sigma_t(\delta)} < -L_{\text{rel}}
\end{aligned}
\end{equation}

where $L_{\text{max}}$ and $L_{\text{rel}}$ are the absolute and relative loss limits, respectively.

\subsubsection{Position Holding Constraints}
To mitigate the risk of positions becoming stale, we impose maximum holding periods:

\begin{equation}
    t - t_{\text{entry}} \leq T_{\text{max}}
\end{equation}

The maximum holding period $T_{\text{max}}$ is calibrated based on the empirical mean reversion time scale of our deviation process.

\subsection{Portfolio-Level Risk Management}

\subsubsection{Value at Risk (VaR) Constraints}
We compute both parametric and historical VaR at confidence level $\alpha$:

\begin{equation}
\begin{aligned}
    \text{VaR}_t^{\text{param}}(\alpha) &= -\mu_t(R^p) - \sigma_t(R^p)\Phi^{-1}(\alpha) \\
    \text{VaR}_t^{\text{hist}}(\alpha) &= -\text{Quantile}\{R_{t-k}^p\}_{k=1}^N(1-\alpha)
\end{aligned}
\end{equation}

Portfolio positions are scaled to ensure:
\begin{equation}
    \text{VaR}_t(\alpha) \leq \text{VaR}_{\text{max}}
\end{equation}

\subsubsection{Market Neutrality Controls}
We maintain market neutrality through several constraints:

\begin{equation}
\begin{aligned}
    \text{Beta Neutrality:} \quad & |\beta_t^M| \leq \beta_{\text{max}} \\
    \text{Dollar Neutrality:} \quad & |P_{0,t} + \mathbf{1}'\mathbf{P}_t| \leq \epsilon
\end{aligned}
\end{equation}

where $\beta_t^M$ is the portfolio's market beta estimated through:
\begin{equation}
    R_t^p = \alpha + \beta_t^M R_t^M + \epsilon_t
\end{equation}

\subsection{Factor Exposure Management}

\subsubsection{Factor Model}
We decompose portfolio risk using a multi-factor model:

\begin{equation}
    R_t^p = \alpha + \sum_{k=1}^K \beta_k F_{k,t} + \epsilon_t
\end{equation}

where $F_{k,t}$ represents the return of factor $k$ at time $t$.

\subsubsection{Factor Exposure Limits}
We impose constraints on factor exposures:

\begin{equation}
    |\beta_k| \leq \beta_{\text{max}}^k \quad \forall k \in \{1,\ldots,K\}
\end{equation}

The aggregate factor risk is controlled through:
\begin{equation}
    \sqrt{\boldsymbol{\beta}'\Sigma_F\boldsymbol{\beta}} \leq \sigma_{\text{max}}^F
\end{equation}

where $\Sigma_F$ is the factor return covariance matrix.

\subsection{Dynamic Risk Assessment}

\subsubsection{Correlation-Based Monitoring}
We continuously monitor the stability of our synthetic control through rolling correlations:

\begin{equation}
    \rho_t = \text{Corr}(R_{0,t}, \hat{R}_{0,t}) = \frac{\text{Cov}_t(R_{0,t}, \hat{R}_{0,t})}{\sigma_t(R_{0,t})\sigma_t(\hat{R}_{0,t})}
\end{equation}

Trading is suspended if:
\begin{equation}
    \rho_t < \rho_{\text{min}}
\end{equation}

\subsubsection{Regime Change Detection}
We implement a regime change detection mechanism using:

\begin{equation}
    D_t = \frac{1}{N}\sum_{s=t-N+1}^t \left(\frac{\delta_s - \mu_t(\delta)}{\sigma_t(\delta)}\right)^2
\end{equation}

Trading is suspended if:
\begin{equation}
    D_t > D_{\text{crit}}
\end{equation}

\subsection{Risk-Adjusted Performance Monitoring}

We continuously monitor risk-adjusted performance metrics:

\begin{equation}
\begin{aligned}
    \text{Rolling Sharpe:} \quad & SR_t = \frac{\hat{\mu}_t(R^s)}{\hat{\sigma}_t(R^s)} \\
    \text{Rolling Sortino:} \quad & SO_t = \frac{\hat{\mu}_t(R^s)}{\hat{\sigma}_t^-(R^s)}
\end{aligned}
\end{equation}

Trading activity is reduced or suspended if:
\begin{equation}
    SR_t < SR_{\text{min}} \quad \text{or} \quad SO_t < SO_{\text{min}}
\end{equation}

This comprehensive risk management framework ensures that our statistical arbitrage strategy maintains controlled exposure to various risk factors while preserving its potential for generating consistent returns.



\section{Performance Analytics}

This section presents a systematic framework for evaluating the performance of our statistical arbitrage strategy, encompassing return attribution, risk metrics, and statistical validation methods.

\subsection{Return Attribution Analysis}

\subsubsection{Return Decomposition}
We decompose the strategy's returns into their constituent components:

\begin{equation}
    R_t^{\text{total}} = R_t^{\text{signal}} + R_t^{\text{execution}} - TC_t
\end{equation}

where:
\begin{equation}
\begin{aligned}
    R_t^{\text{signal}} &= \eta_t\phi_t(R_{0,t} - \hat{R}_{0,t}) \\
    R_t^{\text{execution}} &= \eta_t\phi_t(\hat{R}_{0,t} - \mathbf{w}_t'\mathbf{R}_t) \\
    TC_t &= \text{Transaction costs as defined previously}
\end{aligned}
\end{equation}

This decomposition allows us to isolate the contribution of our signal generation process from implementation effects.

\subsubsection{Performance Metrics}
We compute a comprehensive set of performance metrics over the evaluation period $[0,T]$:

\begin{equation}
\begin{aligned}
    \text{Annualized Return} &= \left(\prod_{t=1}^T (1 + R_t^{\text{total}})\right)^{252/T} - 1 \\
    \text{Annualized Volatility} &= \sqrt{252} \cdot \sqrt{\frac{1}{T-1}\sum_{t=1}^T (R_t^{\text{total}} - \bar{R}^{\text{total}})^2} \\
    \text{Sharpe Ratio} &= \frac{\mathbb{E}[R_t^{\text{total}} - R_f]}{\sigma(R_t^{\text{total}})} \cdot \sqrt{252} \\
    \text{Sortino Ratio} &= \frac{\mathbb{E}[R_t^{\text{total}} - R_f]}{\sigma^-(R_t^{\text{total}})} \cdot \sqrt{252}
\end{aligned}
\end{equation}

\subsection{Risk-Adjusted Performance Measures}

\subsubsection{Drawdown Analysis}
We analyze the depth and duration of drawdowns:

\begin{equation}
\begin{aligned}
    \text{Maximum Drawdown} &= \max_{t,s\leq t}\left(\frac{V_s - V_t}{V_s}\right) \\
    \text{Average Drawdown} &= \frac{1}{T}\sum_{t=1}^T \left(\frac{V_{\text{peak}(t)} - V_t}{V_{\text{peak}(t)}}\right) \\
    \text{Calmar Ratio} &= \frac{\mathbb{E}[R_t^{\text{total}}]}{\text{Maximum Drawdown}}
\end{aligned}
\end{equation}

where $V_{\text{peak}(t)}$ represents the maximum portfolio value achieved prior to time $t$.

\subsubsection{Higher Moment Analysis}
We examine the higher moments of the return distribution:

\begin{equation}
\begin{aligned}
    \text{Skewness} &= \frac{\mathbb{E}[(R_t^{\text{total}} - \mu)^3]}{\sigma^3} \\
    \text{Excess Kurtosis} &= \frac{\mathbb{E}[(R_t^{\text{total}} - \mu)^4]}{\sigma^4} - 3 \\
    \text{Modified VaR} &= \mu + \sigma\left(-z_\alpha + \frac{1}{6}(z_\alpha^2-1)S + \frac{1}{24}(z_\alpha^3-3z_\alpha)K\right)
\end{aligned}
\end{equation}

where $S$ and $K$ represent skewness and excess kurtosis, respectively.

\subsection{Strategy Capacity Analysis}

\subsubsection{Market Impact Assessment}
We estimate strategy capacity through market impact analysis:

\begin{equation}
    R_t^{\text{adjusted}} = R_t^{\text{total}} - \gamma\left(\frac{AUM_t}{\text{ADV}}\right)^\alpha
\end{equation}

where:
\begin{itemize}
    \item $AUM_t$ is assets under management
    \item $\text{ADV}$ is average daily volume
    \item $\gamma$ and $\alpha$ are market impact parameters
\end{itemize}

\subsubsection{Capacity Constraints}
We determine the optimal strategy size by solving:

\begin{equation}
    {AUM}^* = \arg\max_{AUM} \left\{SR(AUM) \quad \text{subject to} \quad R_t^{\text{adjusted}}(AUM) > R_{\text{min}}\right\}
\end{equation}

\subsection{Statistical Validation}

\subsubsection{Hypothesis Testing}
We conduct statistical tests to validate strategy performance:

\begin{equation}
\begin{aligned}
    H_0&: \mathbb{E}[R_t^{\text{total}}] = 0 \\
    H_1&: \mathbb{E}[R_t^{\text{total}}] > 0
\end{aligned}
\end{equation}

The test statistic is:
\begin{equation}
    t = \frac{\bar{R}^{\text{total}}}{\hat{\sigma}/\sqrt{T}} \sim t_{T-1}
\end{equation}

\subsubsection{Robustness Analysis}
We assess strategy robustness through:

\begin{equation}
\begin{aligned}
    \text{Information Ratio} &= \frac{\mathbb{E}[R_t^{\text{total}}]}{\sigma(R_t^{\text{total}} - R_t^b)} \\
    \text{Factor-Adjusted Alpha} &= \alpha_t \text{ from regression on risk factors} \\
    \text{Hit Ratio} &= \mathbb{P}(\text{sign}(R_t^{\text{total}}) = \text{sign}(R_t^{\text{signal}}))
\end{aligned}
\end{equation}

\subsection{Transaction Cost Analysis}

We analyze the impact of transaction costs through:

\begin{equation}
\begin{aligned}
    \text{Cost Ratio} &= \frac{\sum_{t=1}^T TC_t}{\sum_{t=1}^T |R_t^{\text{signal}}|} \\
    \text{Implementation Shortfall} &= R_t^{\text{signal}} - R_t^{\text{total}} \\
    \text{Cost-Adjusted Sharpe} &= \frac{\mathbb{E}[R_t^{\text{total}}]}{\sigma(R_t^{\text{total}})} \cdot \sqrt{252}
\end{aligned}
\end{equation}

This comprehensive performance analysis framework allows us to assess the strategy's effectiveness, robustness, and practical implementability across various market conditions and time horizons.
