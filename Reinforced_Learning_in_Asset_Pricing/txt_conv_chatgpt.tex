\section{Theoretical Framework}

\subsection{Introduction}
In this section, we extend the model of Chen, Pelger, and Zhu (CPZ) (2021) titled "Deep Learning in Asset Pricing" by introducing a Deep Reinforcement Learning (DRL) framework to address the dynamic nature of risk premia in financial markets. The idea is to minimize asset pricing errors by employing Reinforcement Learning (RL) techniques, which iteratively improve the asset pricing model by maximizing the reward function associated with pricing accuracy. This approach takes inspiration from CPZ's adversarial Generative Adversarial Network (GAN) model but expands its capabilities to capture time-varying market conditions more effectively.

\subsection{No-Arbitrage Asset Pricing}

We begin by briefly revisiting the no-arbitrage condition as presented in CPZ. The fundamental assumption of no-arbitrage in financial markets implies the existence of a strictly positive stochastic discount factor (SDF), $M_{t+1}$, such that the expected excess return of any asset $i$ satisfies the following condition:
\begin{equation}
    \mathbb{E}_t[M_{t+1} R_{t+1,i}^e] = 0, 
\end{equation}
where $R_{t+1,i}^e = R_{t+1,i} - R_{t+1}^f$ represents the excess return of asset $i$ over the risk-free rate.

The SDF $M_{t+1}$ can be formulated as an affine function of the tangency portfolio:
\begin{equation}
    M_{t+1} = 1 - \omega_t^\top R_{t+1}^e,
\end{equation}
where $\omega_t$ represents the portfolio weights of the conditional mean-variance efficient portfolio. The fundamental pricing equation implies that the weights are given by:
\begin{equation}
    \omega_t = \mathbb{E}_t[R_{t+1}^e R_{t+1}^e]^{-1} \mathbb{E}_t[R_{t+1}^e].
\end{equation}

The goal of the DRL framework is to learn these optimal portfolio weights and risk loadings iteratively by maximizing the reward function, which is based on minimizing the pricing errors.

\subsection{Reinforcement Learning Framework for Asset Pricing}

The asset pricing problem can be formulated as a Reinforcement Learning problem, where an agent (the model) interacts with an environment (the financial market) to determine an optimal policy (portfolio weights and risk loadings) that maximizes the reward (minimizes pricing errors). In this context, we define the key components of the RL framework as follows:

\begin{itemize}
    \item \textbf{State} ($s_t$): The state at time $t$ consists of the information set, which includes both macroeconomic variables $I_t$ and firm-specific characteristics $I_{t,i}$. Thus, $s_t = (I_t, I_{t,i})$.
    \item \textbf{Action} ($a_t$): The action at time $t$ represents the choice of portfolio weights $\omega_t$ and risk loadings $\beta_t$.
    \item \textbf{Reward} ($r_t$): The reward function is defined as the negative of the pricing error at time $t$, which can be expressed as:
    \begin{equation}
        r_t = - \left\| R_{t+1}^e - \beta_t F_{t+1} \right\|^2,
    \end{equation}
    where $F_{t+1}$ is the systematic factor implied by the SDF.
    \item \textbf{Policy} ($\pi$): The policy $\pi$ represents the mapping from states to actions that maximizes the expected cumulative reward.
\end{itemize}

The agent's objective is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative reward over time:
\begin{equation}
    \pi^* = \arg \max_\pi \mathbb{E}\left[ \sum_{t=0}^T \gamma^t r_t \right],
\end{equation}
where $\gamma \in [0,1]$ is the discount factor that controls the weight of future rewards.

\subsection{Deep Reinforcement Learning for SDF Estimation}

To implement the RL framework, we employ Deep Q-Learning (DQL) as our learning algorithm. DQL is suitable for this problem due to the high dimensionality of the state space and the non-linearity of the relationships between macroeconomic and firm-specific variables and asset returns.

The Q-function $Q(s_t, a_t)$ represents the expected cumulative reward obtained by taking action $a_t$ in state $s_t$ and following the optimal policy thereafter. The Q-function is approximated using a neural network, denoted as $Q_\theta(s_t, a_t)$, with parameters $\theta$. The update rule for the Q-function is given by the Bellman equation:
\begin{equation}
    Q_\theta(s_t, a_t) \leftarrow Q_\theta(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q_\theta(s_{t+1}, a') - Q_\theta(s_t, a_t) \right],
\end{equation}
where $\alpha$ is the learning rate.

The portfolio weights $\omega_t$ and risk loadings $\beta_t$ are updated iteratively based on the learned Q-function. The agent selects actions that maximize the Q-value in each state, thereby minimizing the pricing errors over time.

\subsection{Model Architecture}

The DRL model consists of two main components:
\begin{itemize}
    \item \textbf{State Representation Network}: A feedforward neural network that takes as input the macroeconomic variables $I_t$ and firm-specific characteristics $I_{t,i}$ and outputs a latent representation of the state $s_t$.
    \item \textbf{Q-Network}: A deep neural network that approximates the Q-function $Q_\theta(s_t, a_t)$. The input to this network is the state representation $s_t$, and the output is the estimated Q-value for each possible action.
\end{itemize}

The model is trained using an experience replay mechanism, where the agent stores past transitions $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer and samples mini-batches from this buffer to update the Q-network. This helps to break the temporal correlation between consecutive transitions and stabilizes the training process.

\subsection{Conclusion}

The proposed DRL framework extends the CPZ model by introducing a dynamic approach to learning the stochastic discount factor and the associated portfolio weights and risk loadings. By iteratively optimizing the policy to minimize pricing errors, the model aims to achieve a more accurate representation of the cross-section of asset returns, capturing time-varying market conditions and systematic risk factors in a flexible manner.