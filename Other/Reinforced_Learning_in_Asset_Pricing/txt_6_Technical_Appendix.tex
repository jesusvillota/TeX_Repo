\section{Technical Appendices}

\subsection*{Appendix A: Reinforcement Learning Algorithm in Asset Pricing}

In this appendix, we provide a detailed explanation of the Reinforcement Learning (RL) algorithms used in our model, with a focus on how they are adapted for asset pricing.

\subsubsection*{Q-Learning}

\textbf{Q-Learning} is a model-free RL algorithm that seeks to find the optimal action-value function, $Q(s, a)$, which gives the expected cumulative reward for taking action $a$ in state $s$ and following the optimal policy thereafter. The Q-Learning update rule is given by:

\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right)
\]
where:
\begin{itemize}
    \item $\alpha$ is the learning rate.
    \item $\gamma$ is the discount factor for future rewards.
    \item $R_t$ is the immediate reward received after taking action $a_t$ in state $s_t$.
    \item $\max_{a'} Q(s_{t+1}, a')$ is the maximum predicted reward for the next state $s_{t+1}$.
\end{itemize}

However, since asset pricing involves a continuous action space (e.g., portfolio weights $\omega_t$ and risk loadings $\beta_t$), discrete action-based algorithms like Q-learning are not directly applicable. To address this, we use actor-critic methods or policy-based algorithms, which are better suited for continuous control problems.

\subsubsection*{Deep Deterministic Policy Gradient (DDPG)}

\textbf{Deep Deterministic Policy Gradient (DDPG)} is an actor-critic method designed for continuous action spaces, such as the portfolio adjustment problem in asset pricing. DDPG combines policy gradient methods with deterministic Q-learning to directly optimize the policy function. The DDPG architecture includes:
\begin{itemize}
    \item \textbf{Actor Network:} Outputs a deterministic action $a_t = \pi(s_t \mid \theta^\pi)$, where $\theta^\pi$ are the parameters of the actor network.
    \item \textbf{Critic Network:} Evaluates the action by outputting a scalar value $Q(s_t, a_t \mid \theta^Q)$, where $\theta^Q$ are the parameters of the critic network.
\end{itemize}

The critic network is updated by minimizing the loss:
\[
L(\theta^Q) = \mathbb{E} \left[ \left( R_t + \gamma Q(s_{t+1}, \pi(s_{t+1} \mid \theta^\pi) \mid \theta^Q) - Q(s_t, a_t \mid \theta^Q) \right)^2 \right]
\]
The actor network is updated by minimizing the negative expected Q-value:
\[
L(\theta^\pi) = -\mathbb{E} \left[ Q(s_t, \pi(s_t \mid \theta^\pi) \mid \theta^Q) \right]
\]

In our asset pricing application, the DDPG algorithm is well-suited for learning optimal portfolio adjustments, as it can handle the continuous action space of portfolio weights $\omega_t$ and risk loadings $\beta_t$. The actor network generates portfolio weight adjustments, and the critic network evaluates the expected return for these adjustments based on the reward function discussed in Section 2.

\subsubsection*{Proximal Policy Optimization (PPO)}

We implement \textbf{Proximal Policy Optimization (PPO)} as the primary RL algorithm due to its stability and efficiency. PPO is a policy gradient method that maximizes the expected reward by updating the policy network while ensuring that updates stay within a safe region to prevent divergence.

The objective function for PPO is defined as:
\[
L^{PPO}(\theta) = \mathbb{E} \left[ \min\left( r_t(\theta) A(s_t, a_t), \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A(s_t, a_t) \right) \right]
\]
where:
\begin{itemize}
    \item $r_t(\theta)$ is the ratio of the new policy to the old policy: $r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$.
    \item $A(s_t, a_t)$ is the advantage function, which measures how much better the current action is compared to the expected action under the current policy.
    \item $\epsilon$ is a hyperparameter that controls the clipping range for the policy update.
\end{itemize}

PPO updates the policy in small steps, ensuring that it does not diverge too far from the previous policy, which is critical in high-variance environments such as financial markets.

\subsection*{Appendix B: Hyperparameter Tuning}

In this appendix, we discuss the process of tuning hyperparameters for the RL model. Hyperparameter tuning is critical to ensure that the RL model is both stable and efficient. The following key hyperparameters were tuned using cross-validation on the validation dataset:

\subsubsection*{Learning Rate}

The learning rate controls the step size of the gradient descent algorithm used to update the actor and critic networks. A learning rate that is too high can lead to instability, while a rate that is too low can slow down convergence. After experimentation, we found that a learning rate in the range of $10^{-4}$ to $10^{-3}$ provided the best balance between convergence speed and stability.

\subsubsection*{Discount Factor ($\gamma$)}

The discount factor $\gamma$ determines how much weight is given to future rewards versus immediate rewards. A higher value of $\gamma$ prioritizes long-term returns, while a lower value focuses more on short-term rewards. In our implementation, a $\gamma$ value of 0.99 worked well, as it gave more importance to long-term performance, which aligns with the goal of optimizing the risk-return tradeoff over time.

\subsubsection*{Batch Size}

Batch size controls the number of experience tuples processed at each step of the gradient update. Smaller batch sizes lead to noisier updates, while larger batch sizes provide more stable updates. We experimented with batch sizes ranging from 32 to 512, and found that a batch size of 128 provided a good balance between stability and computational efficiency.

\subsubsection*{Exploration Parameters}

Exploration is a key component of RL, where the agent needs to explore different actions to discover the optimal policy. In our model, we use an $\epsilon$-greedy policy for exploration, where $\epsilon$ is the probability of selecting a random action. We start with $\epsilon = 0.1$, meaning the agent explores 10\% of the time, and decay this parameter over time to encourage exploitation of learned strategies.

\subsubsection*{Number of Layers and Neurons}

The architecture of the actor and critic networks was tuned by experimenting with different numbers of layers and neurons. After cross-validation, we found that using two hidden layers with 128 neurons each provided the best performance for both networks.

\subsection*{Appendix C: Simulation Results Across Market Conditions}

In this appendix, we present the simulation results comparing the RL, GAN, and Feedforward Network (FFN) models under different market conditions. The models were tested in three distinct market environments:
\begin{itemize}
    \item \textbf{Bull Market}: Characterized by steadily rising asset prices and low volatility.
    \item \textbf{Bear Market}: Characterized by falling asset prices and high volatility.
    \item \textbf{Sideways Market}: Characterized by little to no trend in asset prices and moderate volatility.
\end{itemize}

\subsubsection*{Bull Market Results}

In a bull market, all models performed relatively well, but the RL model outperformed both the GAN and FFN models in terms of risk-adjusted returns. The RL model achieved a Sharpe ratio of 1.80, compared to 1.65 for the GAN model and 1.50 for the FFN.

\subsubsection*{Bear Market Results}

In the bear market simulation, the RL model significantly outperformed the other models due to its ability to dynamically adjust risk exposures in response to increased volatility. The RL model achieved a Sharpe ratio of 1.40, while the GAN and FFN models achieved ratios of 1.10 and 1.05, respectively.

\subsubsection*{Sideways Market Results}

In the sideways market, where asset prices showed little directional movement, the RL model maintained higher returns and lower volatility compared to the GAN and FFN models. The RL model achieved a Sharpe ratio of 1.60, compared to 1.35 for the GAN and 1.20 for the FFN.

\subsubsection*{Discussion of Results}

Across all market conditions, the RL model consistently outperformed both the GAN and FFN models. Its ability to adapt dynamically to changing market environments and learn optimal policies for portfolio adjustment made it more robust across a range of market conditions. The GAN model, while competitive, struggled in environments with high volatility (bear markets), and the FFN model, which uses a static approach, underperformed in all scenarios.

\newpage
